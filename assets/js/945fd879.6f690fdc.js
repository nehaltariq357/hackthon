"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[833],{5849:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var s=t(4848),o=t(8453),i=t(1226),a=t(5783);const r={sidebar_position:2,title:"Humanoid Robot Integration"},c="Humanoid Robot Integration",l={id:"capstone-project/topic-1",title:"Humanoid Robot Integration",description:"The capstone project integrates all concepts from previous modules into a complete autonomous humanoid robot system. This module focuses on the integration aspects of combining vision, language understanding, and action execution in a humanoid form factor.",source:"@site/docs/capstone-project/topic-1.mdx",sourceDirName:"capstone-project",slug:"/capstone-project/topic-1",permalink:"/docs/capstone-project/topic-1",draft:!1,unlisted:!1,editUrl:"https://github.com/nehaltariq357/hackthon/tree/master/docs/capstone-project/topic-1.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Humanoid Robot Integration"},sidebar:"textbookSidebar",previous:{title:"Capstone Project: Autonomous Humanoid Robot",permalink:"/docs/capstone-project/"},next:{title:"Autonomous Behavior Implementation",permalink:"/docs/capstone-project/topic-2"}},d={},p=[{value:"Overview of Humanoid Robot Architecture",id:"overview-of-humanoid-robot-architecture",level:2},{value:"Humanoid Control Architecture",id:"humanoid-control-architecture",level:2},{value:"Humanoid Perception System",id:"humanoid-perception-system",level:2},{value:"Integration Framework",id:"integration-framework",level:2},{value:"Best Practices for Humanoid Integration",id:"best-practices-for-humanoid-integration",level:2},{value:"Summary",id:"summary",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.A,{}),"\n",(0,s.jsx)(n.h1,{id:"humanoid-robot-integration",children:"Humanoid Robot Integration"}),"\n",(0,s.jsx)(n.p,{children:"The capstone project integrates all concepts from previous modules into a complete autonomous humanoid robot system. This module focuses on the integration aspects of combining vision, language understanding, and action execution in a humanoid form factor."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-humanoid-robot-architecture",children:"Overview of Humanoid Robot Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A humanoid robot system integrates multiple complex subsystems:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Locomotion"}),": Walking, balancing, and movement control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation"}),": Arm and hand control for object interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Vision, audio, and sensor fusion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cognition"}),": Planning, reasoning, and decision making"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Communication"}),": Natural language and social interaction"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"humanoid-control-architecture",children:"Humanoid Control Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="humanoid_robot_controller.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32\nfrom sensor_msgs.msg import JointState, Image, Imu, LaserScan\nfrom geometry_msgs.msg import Twist, Pose, PointStamped\nfrom builtin_interfaces.msg import Duration\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.msg import JointTrajectoryControllerState\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nfrom typing import Dict, List, Optional, Tuple\n\nclass HumanoidRobotController(Node):\n    def __init__(self):\n        super().__init__('humanoid_robot_controller')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers for different subsystems\n        self.joint_trajectory_pub = self.create_publisher(\n            JointTrajectory, 'joint_trajectory_controller/joint_trajectory', 10)\n        self.cmd_vel_pub = self.create_publisher(\n            Twist, 'cmd_vel', 10)\n        self.speech_pub = self.create_publisher(\n            String, 'tts_input', 10)\n\n        # Subscribers for sensor data\n        self.joint_state_sub = self.create_subscription(\n            JointState, 'joint_states', self.joint_state_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, 'imu/data', self.imu_callback, 10)\n        self.camera_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.camera_callback, 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, 'scan', self.laser_callback, 10)\n\n        # Humanoid-specific parameters\n        self.joint_names = [\n            'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',\n            'right_hip_joint', 'right_knee_joint', 'right_ankle_joint',\n            'left_shoulder_joint', 'left_elbow_joint', 'left_wrist_joint',\n            'right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint',\n            'head_pan_joint', 'head_tilt_joint'\n        ]\n\n        # Robot state\n        self.current_joint_positions = {}\n        self.imu_data = None\n        self.camera_image = None\n        self.laser_data = None\n        self.robot_pose = Pose()\n\n        # Walking controller parameters\n        self.walk_params = {\n            'step_height': 0.05,\n            'step_length': 0.3,\n            'step_duration': 1.0,\n            'balance_threshold': 0.1\n        }\n\n        # Timer for humanoid control loop\n        self.control_timer = self.create_timer(0.05, self.control_loop)\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update joint positions\"\"\"\n        for i, name in enumerate(msg.name):\n            if name in self.joint_names:\n                self.current_joint_positions[name] = msg.position[i]\n\n    def imu_callback(self, msg):\n        \"\"\"Update IMU data for balance control\"\"\"\n        self.imu_data = {\n            'orientation': (msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w),\n            'angular_velocity': (msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z),\n            'linear_acceleration': (msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z)\n        }\n\n    def camera_callback(self, msg):\n        \"\"\"Update camera image\"\"\"\n        try:\n            self.camera_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        except Exception as e:\n            self.get_logger().warn(f'Error processing camera image: {e}')\n\n    def laser_callback(self, msg):\n        \"\"\"Update laser scan data\"\"\"\n        self.laser_data = msg.ranges\n\n    def control_loop(self):\n        \"\"\"Main humanoid control loop\"\"\"\n        # Check balance using IMU data\n        if self.imu_data:\n            self.check_balance()\n\n        # Process any pending high-level commands\n        self.process_high_level_commands()\n\n    def check_balance(self):\n        \"\"\"Check robot balance using IMU data\"\"\"\n        if self.imu_data:\n            # Extract orientation from quaternion\n            quat = self.imu_data['orientation']\n            roll, pitch, yaw = self.quaternion_to_euler(quat)\n\n            # Check if robot is within balance thresholds\n            if abs(roll) > self.walk_params['balance_threshold'] or abs(pitch) > self.walk_params['balance_threshold']:\n                self.get_logger().warn(f'Balance threshold exceeded: roll={roll:.3f}, pitch={pitch:.3f}')\n                # Trigger balance recovery\n                self.recover_balance()\n\n    def quaternion_to_euler(self, quat: Tuple[float, float, float, float]) -> Tuple[float, float, float]:\n        \"\"\"Convert quaternion to Euler angles\"\"\"\n        x, y, z, w = quat\n\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range\n        else:\n            pitch = math.asin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n\n    def recover_balance(self):\n        \"\"\"Recover robot balance\"\"\"\n        # Publish zero joint commands to stop movement\n        zero_trajectory = self.create_joint_trajectory({joint: 0.0 for joint in self.joint_names})\n        self.joint_trajectory_pub.publish(zero_trajectory)\n\n        # Stop any base movement\n        stop_cmd = Twist()\n        self.cmd_vel_pub.publish(stop_cmd)\n\n        self.get_logger().info('Balance recovery initiated')\n\n    def walk_forward(self, distance: float):\n        \"\"\"Execute walking motion for specified distance\"\"\"\n        # Calculate number of steps needed\n        num_steps = int(distance / self.walk_params['step_length'])\n\n        for step in range(num_steps):\n            # Execute one walking step\n            self.execute_walking_step()\n\n    def execute_walking_step(self):\n        \"\"\"Execute a single walking step\"\"\"\n        # This is a simplified walking pattern\n        # Real humanoid walking requires complex gait planning\n\n        # Example: Simple alternating leg movement\n        step_trajectory = JointTrajectory()\n        step_trajectory.joint_names = self.joint_names\n\n        # Create trajectory points for the step\n        point = JointTrajectoryPoint()\n\n        # Set positions for a step (simplified)\n        positions = []\n        for joint_name in self.joint_names:\n            if 'hip' in joint_name:\n                positions.append(0.1 if 'left' in joint_name else -0.1)  # Lift leg\n            elif 'knee' in joint_name:\n                positions.append(0.2 if 'left' in joint_name else -0.2)  # Bend knee\n            elif 'ankle' in joint_name:\n                positions.append(0.05 if 'left' in joint_name else -0.05)  # Adjust ankle\n            else:\n                positions.append(0.0)  # Keep other joints neutral\n\n        point.positions = positions\n        point.time_from_start = Duration(sec=int(self.walk_params['step_duration']))\n        step_trajectory.points = [point]\n\n        self.joint_trajectory_pub.publish(step_trajectory)\n\n    def wave_hand(self):\n        \"\"\"Execute waving gesture\"\"\"\n        wave_trajectory = JointTrajectory()\n        wave_trajectory.joint_names = ['right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint']\n\n        points = []\n\n        # Point 1: Initial position\n        p1 = JointTrajectoryPoint()\n        p1.positions = [0.0, 0.0, 0.0]\n        p1.time_from_start = Duration(sec=0, nanosec=500000000)\n        points.append(p1)\n\n        # Point 2: Raise arm\n        p2 = JointTrajectoryPoint()\n        p2.positions = [0.5, 0.5, 0.0]\n        p2.time_from_start = Duration(sec=1)\n        points.append(p2)\n\n        # Point 3: Wave motion\n        p3 = JointTrajectoryPoint()\n        p3.positions = [0.5, 0.5, 0.5]\n        p3.time_from_start = Duration(sec=1, nanosec=500000000)\n        points.append(p3)\n\n        # Point 4: Return to position\n        p4 = JointTrajectoryPoint()\n        p4.positions = [0.5, 0.5, -0.5]\n        p4.time_from_start = Duration(sec=2)\n        points.append(p4)\n\n        # Point 5: Lower arm\n        p5 = JointTrajectoryPoint()\n        p5.positions = [0.0, 0.0, 0.0]\n        p5.time_from_start = Duration(sec=2, nanosec=500000000)\n        points.append(p5)\n\n        wave_trajectory.points = points\n        self.joint_trajectory_pub.publish(wave_trajectory)\n\n    def speak(self, text: str):\n        \"\"\"Make robot speak text\"\"\"\n        speech_msg = String()\n        speech_msg.data = text\n        self.speech_pub.publish(speech_msg)\n\n    def create_joint_trajectory(self, joint_positions: Dict[str, float]) -> JointTrajectory:\n        \"\"\"Create a joint trajectory message\"\"\"\n        trajectory = JointTrajectory()\n        trajectory.joint_names = list(joint_positions.keys())\n\n        point = JointTrajectoryPoint()\n        point.positions = list(joint_positions.values())\n        point.time_from_start = Duration(sec=1)  # 1 second to reach position\n\n        trajectory.points = [point]\n        return trajectory\n\n    def process_high_level_commands(self):\n        \"\"\"Process high-level commands from VLA system\"\"\"\n        # This would interface with the VLA system to execute commands\n        # For demo, we'll just implement some basic commands\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    humanoid_controller = HumanoidRobotController()\n\n    try:\n        rclpy.spin(humanoid_controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        humanoid_controller.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"humanoid-perception-system",children:"Humanoid Perception System"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="humanoid_perception_system.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan, Imu\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point, Vector3\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport open3d as o3d\nfrom typing import List, Dict, Any, Optional\n\nclass HumanoidPerceptionSystem(Node):\n    def __init__(self):\n        super().__init__('humanoid_perception_system')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, 'perception/detections', 10)\n        self.spatial_detection_pub = self.create_publisher(\n            Detection2DArray, 'perception/spatial_detections', 10)\n\n        # Subscribers\n        self.rgb_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.rgb_callback, 10)\n        self.depth_sub = self.create_subscription(\n            Image, 'camera/depth/image_raw', self.depth_callback, 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, 'scan', self.laser_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, 'imu/data', self.imu_callback, 10)\n\n        # Internal state\n        self.rgb_image = None\n        self.depth_image = None\n        self.laser_data = None\n        self.imu_data = None\n\n        # Object detection model (using OpenCV DNN as example)\n        self.detection_model = cv2.dnn.readNetFromDarknet(\n            '/path/to/yolo.cfg', '/path/to/yolo.weights'\n        )\n        self.detection_classes = self.load_coco_classes()\n\n    def load_coco_classes(self) -> List[str]:\n        \"\"\"Load COCO dataset class names\"\"\"\n        # In practice, load from file\n        return [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def rgb_callback(self, msg):\n        \"\"\"Process RGB image\"\"\"\n        try:\n            self.rgb_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Run object detection\n            detections = self.run_object_detection(self.rgb_image)\n\n            # Publish detections\n            self.publish_detections(detections, msg.header)\n\n            # If we have depth data, create spatial detections\n            if self.depth_image is not None:\n                spatial_detections = self.create_spatial_detections(detections, self.depth_image)\n                self.publish_spatial_detections(spatial_detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing RGB image: {e}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image\"\"\"\n        try:\n            self.depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n        except Exception as e:\n            self.get_logger().error(f'Error processing depth image: {e}')\n\n    def laser_callback(self, msg):\n        \"\"\"Process laser scan data\"\"\"\n        self.laser_data = msg.ranges\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        self.imu_data = msg\n\n    def run_object_detection(self, image: np.ndarray) -> List[Dict[str, Any]]:\n        \"\"\"Run object detection on image\"\"\"\n        height, width = image.shape[:2]\n\n        # Create blob from image\n        blob = cv2.dnn.blobFromImage(\n            image, 1/255.0, (416, 416), swapRB=True, crop=False\n        )\n\n        # Set input to the model\n        self.detection_model.setInput(blob)\n\n        # Run forward pass\n        layer_names = self.detection_model.getLayerNames()\n        output_names = [layer_names[i[0] - 1] for i in self.detection_model.getUnconnectedOutLayers()]\n        outputs = self.detection_model.forward(output_names)\n\n        # Process outputs\n        boxes = []\n        confidences = []\n        class_ids = []\n\n        for output in outputs:\n            for detection in output:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > 0.5:  # Threshold\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n\n        detections = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                x, y, w, h = boxes[i]\n                class_id = class_ids[i]\n                confidence = confidences[i]\n\n                detection = {\n                    'bbox': [x, y, w, h],\n                    'class_id': class_id,\n                    'class_name': self.detection_classes[class_id] if class_id < len(self.detection_classes) else 'unknown',\n                    'confidence': confidence\n                }\n                detections.append(detection)\n\n        return detections\n\n    def create_spatial_detections(self, detections: List[Dict[str, Any]], depth_image: np.ndarray) -> List[Dict[str, Any]]:\n        \"\"\"Add spatial information to detections using depth image\"\"\"\n        spatial_detections = []\n\n        for detection in detections:\n            x, y, w, h = detection['bbox']\n\n            # Calculate center of bounding box\n            center_x = x + w // 2\n            center_y = y + h // 2\n\n            # Get depth at center of bounding box (average over a small region)\n            depth_region = depth_image[center_y-10:center_y+10, center_x-10:center_x+10]\n            depth_value = np.nanmean(depth_region[depth_region > 0])  # Ignore invalid depth values\n\n            spatial_detection = detection.copy()\n            spatial_detection['distance'] = float(depth_value) if not np.isnan(depth_value) else 0.0\n\n            # Calculate approximate 3D position (simplified)\n            # In practice, use camera intrinsic parameters for accurate calculation\n            spatial_detection['position_3d'] = {\n                'x': float(depth_value),  # Simplified - in practice, use proper camera model\n                'y': float((center_x - depth_image.shape[1]/2) * depth_value / 500),  # Approximate\n                'z': float((center_y - depth_image.shape[0]/2) * depth_value / 500)   # Approximate\n            }\n\n            spatial_detections.append(spatial_detection)\n\n        return spatial_detections\n\n    def publish_detections(self, detections: List[Dict[str, Any]], header):\n        \"\"\"Publish detections as ROS message\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        for detection in detections:\n            detection_msg = Detection2D()\n            detection_msg.header = header\n\n            # Bounding box\n            bbox = BoundingBox2D()\n            bbox.center.position.x = detection['bbox'][0] + detection['bbox'][2] / 2\n            bbox.center.position.y = detection['bbox'][1] + detection['bbox'][3] / 2\n            bbox.size_x = detection['bbox'][2]\n            bbox.size_y = detection['bbox'][3]\n            detection_msg.bbox = bbox\n\n            # Results\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = detection['class_name']\n            result.hypothesis.score = detection['confidence']\n            detection_msg.results.append(result)\n\n            detections_msg.detections.append(detection_msg)\n\n        self.detection_pub.publish(detections_msg)\n\n    def publish_spatial_detections(self, spatial_detections: List[Dict[str, Any]], header):\n        \"\"\"Publish spatial detections as ROS message\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        for detection in spatial_detections:\n            detection_msg = Detection2D()\n            detection_msg.header = header\n\n            # Bounding box\n            bbox = BoundingBox2D()\n            bbox.center.position.x = detection['bbox'][0] + detection['bbox'][2] / 2\n            bbox.center.position.y = detection['bbox'][1] + detection['bbox'][3] / 2\n            bbox.size_x = detection['bbox'][2]\n            bbox.size_y = detection['bbox'][3]\n            detection_msg.bbox = bbox\n\n            # Add distance as a key-value pair in the result\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = detection['class_name']\n            result.hypothesis.score = detection['confidence']\n            detection_msg.results.append(result)\n\n            # Store 3D position in the detection message (custom extension)\n            # In practice, you might use a custom message type for spatial information\n\n            detections_msg.detections.append(detection_msg)\n\n        self.spatial_detection_pub.publish(detections_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_system = HumanoidPerceptionSystem()\n\n    try:\n        rclpy.spin(perception_system)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-framework",children:"Integration Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="humanoid_integration_framework.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState, Image, Imu\nfrom geometry_msgs.msg import Twist, Pose\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom threading import Lock\nimport json\nfrom typing import Dict, Any, Optional\n\nclass HumanoidIntegrationFramework(Node):\n    def __init__(self):\n        super().__init__('humanoid_integration_framework')\n\n        # Publishers and subscribers\n        self.task_command_sub = self.create_subscription(\n            String, 'capstone_task_command', self.task_command_callback, 10)\n        self.status_pub = self.create_publisher(\n            String, 'integration_status', 10)\n\n        # Component interfaces\n        self.humanoid_controller = None\n        self.perception_system = None\n        self.vla_system = None\n\n        # Internal state\n        self.current_task = None\n        self.task_status = \"idle\"\n        self.system_status = {\n            'locomotion': 'ready',\n            'manipulation': 'ready',\n            'perception': 'ready',\n            'cognition': 'ready'\n        }\n\n        # Thread safety\n        self.state_lock = Lock()\n\n        # Timer for integration monitoring\n        self.monitor_timer = self.create_timer(1.0, self.monitor_system)\n\n    def task_command_callback(self, msg):\n        \"\"\"Handle high-level capstone tasks\"\"\"\n        try:\n            task_data = json.loads(msg.data)\n            task_type = task_data.get('type', 'unknown')\n            task_params = task_data.get('parameters', {})\n\n            self.get_logger().info(f'Received capstone task: {task_type}')\n\n            # Execute the task\n            self.execute_capstone_task(task_type, task_params)\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in task command: {msg.data}')\n        except Exception as e:\n            self.get_logger().error(f'Error processing task command: {e}')\n\n    def execute_capstone_task(self, task_type: str, params: Dict[str, Any]):\n        \"\"\"Execute a capstone task\"\"\"\n        with self.state_lock:\n            self.current_task = task_type\n            self.task_status = \"executing\"\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f'EXECUTING: {task_type}'\n        self.status_pub.publish(status_msg)\n\n        # Execute based on task type\n        if task_type == 'greet_and_guide':\n            self.execute_greet_and_guide_task(params)\n        elif task_type == 'fetch_and_carry':\n            self.execute_fetch_and_carry_task(params)\n        elif task_type == 'navigate_and_interact':\n            self.execute_navigate_and_interact_task(params)\n        else:\n            self.get_logger().warn(f'Unknown capstone task: {task_type}')\n            self.complete_task(False, f'Unknown task type: {task_type}')\n\n    def execute_greet_and_guide_task(self, params: Dict[str, Any]):\n        \"\"\"Execute greeting and guiding task\"\"\"\n        self.get_logger().info('Executing greet and guide task')\n\n        try:\n            # 1. Detect person\n            person_detected = self.detect_person()\n            if not person_detected:\n                self.complete_task(False, 'No person detected')\n                return\n\n            # 2. Greet person\n            self.greet_person()\n\n            # 3. Listen for destination\n            destination = self.listen_for_destination()\n            if not destination:\n                self.complete_task(False, 'No destination provided')\n                return\n\n            # 4. Navigate to destination while providing guidance\n            success = self.navigate_with_guidance(destination)\n            self.complete_task(success, f'Navigation {\"succeeded\" if success else \"failed\"}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in greet and guide task: {e}')\n            self.complete_task(False, f'Task failed: {e}')\n\n    def execute_fetch_and_carry_task(self, params: Dict[str, Any]):\n        \"\"\"Execute fetch and carry task\"\"\"\n        self.get_logger().info('Executing fetch and carry task')\n\n        try:\n            # 1. Navigate to source location\n            source_location = params.get('source', 'kitchen')\n            success = self.navigate_to_location(source_location)\n            if not success:\n                self.complete_task(False, f'Failed to navigate to {source_location}')\n                return\n\n            # 2. Detect and pick up object\n            object_to_fetch = params.get('object', 'cup')\n            success = self.detect_and_pick_up_object(object_to_fetch)\n            if not success:\n                self.complete_task(False, f'Failed to pick up {object_to_fetch}')\n                return\n\n            # 3. Navigate to destination\n            destination = params.get('destination', 'living_room')\n            success = self.navigate_to_location(destination)\n            if not success:\n                self.complete_task(False, f'Failed to navigate to {destination}')\n                return\n\n            # 4. Place object down\n            success = self.place_object_down()\n            self.complete_task(success, f'Fetch task {\"succeeded\" if success else \"failed\"}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in fetch and carry task: {e}')\n            self.complete_task(False, f'Task failed: {e}')\n\n    def execute_navigate_and_interact_task(self, params: Dict[str, Any]):\n        \"\"\"Execute navigation and interaction task\"\"\"\n        self.get_logger().info('Executing navigate and interact task')\n\n        try:\n            # 1. Navigate to location\n            location = params.get('location', 'kitchen')\n            success = self.navigate_to_location(location)\n            if not success:\n                self.complete_task(False, f'Failed to navigate to {location}')\n                return\n\n            # 2. Detect objects in the area\n            objects = self.detect_objects_in_area()\n\n            # 3. Interact based on detected objects\n            interaction_success = self.interact_with_objects(objects)\n            self.complete_task(interaction_success, f'Interaction task {\"succeeded\" if interaction_success else \"failed\"}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in navigate and interact task: {e}')\n            self.complete_task(False, f'Task failed: {e}')\n\n    def detect_person(self) -> bool:\n        \"\"\"Detect if a person is present\"\"\"\n        # This would interface with perception system\n        # For demo, return True\n        self.get_logger().info('Person detected')\n        return True\n\n    def greet_person(self):\n        \"\"\"Greet the detected person\"\"\"\n        # Interface with speech system\n        self.get_logger().info('Greeting person')\n        # self.speak(\"Hello! How can I help you today?\")\n\n    def listen_for_destination(self) -> Optional[str]:\n        \"\"\"Listen for destination from person\"\"\"\n        # This would interface with speech recognition\n        # For demo, return a fixed destination\n        self.get_logger().info('Listening for destination')\n        return \"kitchen\"\n\n    def navigate_with_guidance(self, destination: str) -> bool:\n        \"\"\"Navigate to destination while providing guidance\"\"\"\n        self.get_logger().info(f'Navigating to {destination} with guidance')\n        # This would implement navigation with periodic status updates\n        return True\n\n    def navigate_to_location(self, location: str) -> bool:\n        \"\"\"Navigate to specified location\"\"\"\n        self.get_logger().info(f'Navigating to {location}')\n        # This would interface with navigation system\n        return True\n\n    def detect_and_pick_up_object(self, object_name: str) -> bool:\n        \"\"\"Detect and pick up specified object\"\"\"\n        self.get_logger().info(f'Detecting and picking up {object_name}')\n        # This would interface with perception and manipulation systems\n        return True\n\n    def place_object_down(self) -> bool:\n        \"\"\"Place currently held object down\"\"\"\n        self.get_logger().info('Placing object down')\n        # This would interface with manipulation system\n        return True\n\n    def detect_objects_in_area(self) -> list:\n        \"\"\"Detect objects in current area\"\"\"\n        self.get_logger().info('Detecting objects in area')\n        # This would interface with perception system\n        return ['table', 'chair', 'cup']\n\n    def interact_with_objects(self, objects: list) -> bool:\n        \"\"\"Interact with detected objects\"\"\"\n        self.get_logger().info(f'Interacting with objects: {objects}')\n        # This would implement appropriate interactions\n        return True\n\n    def complete_task(self, success: bool, message: str):\n        \"\"\"Complete the current task\"\"\"\n        with self.state_lock:\n            self.task_status = \"completed\"\n            self.current_task = None\n\n        status_msg = String()\n        status_msg.data = f'{\"SUCCESS\" if success else \"FAILURE\"}: {message}'\n        self.status_pub.publish(status_msg)\n\n        self.get_logger().info(f'Task completed with status: {\"SUCCESS\" if success else \"FAILURE\"} - {message}')\n\n    def monitor_system(self):\n        \"\"\"Monitor the overall system status\"\"\"\n        # Check if all components are operational\n        operational_status = \"OPERATIONAL\" if all(\n            status == 'ready' for status in self.system_status.values()\n        ) else \"DEGRADED\"\n\n        status_msg = String()\n        status_msg.data = f'System: {operational_status}, Task: {self.task_status}'\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    integration_framework = HumanoidIntegrationFramework()\n\n    try:\n        rclpy.spin(integration_framework)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        integration_framework.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-humanoid-integration",children:"Best Practices for Humanoid Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modularity"}),": Keep subsystems as independent modules that can be tested separately"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Implement multiple safety layers and emergency stops"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Calibration"}),": Regularly calibrate sensors and actuators"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing"}),": Extensive testing in simulation before real-world deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Continuous monitoring of all subsystems"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robot integration combines all the concepts learned in previous modules into a complete autonomous system. The integration requires careful coordination between locomotion, manipulation, perception, and cognition subsystems. Success depends on modular design, robust safety mechanisms, and thorough testing."}),"\n",(0,s.jsx)(i.A,{title:"Humanoid Robot Integration",description:"Shows the integration of all subsystems in a humanoid robot platform"})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(_,{...e})}):_(e)}}}]);