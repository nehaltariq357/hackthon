"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[448],{9215:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>_});var s=t(4848),i=t(8453),a=t(1226),o=t(5783);const r={sidebar_position:4,title:"Action Planning and Execution"},c="Action Planning and Execution",l={id:"module-4-vla/topic-3",title:"Action Planning and Execution",description:"Action planning and execution form the final component of Vision-Language-Action (VLA) systems, where high-level commands are translated into specific robot behaviors. This module covers planning algorithms, execution frameworks, and integration with VLA systems.",source:"@site/docs/module-4-vla/topic-3.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/topic-3",permalink:"/docs/module-4-vla/topic-3",draft:!1,unlisted:!1,editUrl:"https://github.com/nehaltariq357/hackthon/tree/master/docs/module-4-vla/topic-3.mdx",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"Action Planning and Execution"},sidebar:"textbookSidebar",previous:{title:"Language Models for Robot Control",permalink:"/docs/module-4-vla/topic-2"},next:{title:"Capstone Project: Autonomous Humanoid Robot",permalink:"/docs/capstone-project/"}},d={},_=[{value:"Overview of Action Planning",id:"overview-of-action-planning",level:2},{value:"Behavior Trees for Action Execution",id:"behavior-trees-for-action-execution",level:2},{value:"State Machines for Complex Actions",id:"state-machines-for-complex-actions",level:2},{value:"Integration with VLA Systems",id:"integration-with-vla-systems",level:2},{value:"Best Practices for Action Planning",id:"best-practices-for-action-planning",level:2},{value:"Summary",id:"summary",level:2}];function f(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(o.A,{}),"\n",(0,s.jsx)(n.h1,{id:"action-planning-and-execution",children:"Action Planning and Execution"}),"\n",(0,s.jsx)(n.p,{children:"Action planning and execution form the final component of Vision-Language-Action (VLA) systems, where high-level commands are translated into specific robot behaviors. This module covers planning algorithms, execution frameworks, and integration with VLA systems."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-action-planning",children:"Overview of Action Planning"}),"\n",(0,s.jsx)(n.p,{children:"Action planning in robotics involves:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Planning"}),": High-level task decomposition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Motion Planning"}),": Path planning and obstacle avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution Monitoring"}),": Real-time execution and error handling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replanning"}),": Dynamic adjustment to changing conditions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"behavior-trees-for-action-execution",children:"Behavior Trees for Action Execution"}),"\n",(0,s.jsx)(n.p,{children:"Behavior trees provide a structured approach to action execution:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="behavior_tree_action_planner.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Twist\nfrom sensor_msgs.msg import LaserScan\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped\nimport math\nfrom enum import Enum\nfrom typing import Optional, List, Dict, Any\n\nclass NodeStatus(Enum):\n    SUCCESS = 1\n    FAILURE = 2\n    RUNNING = 3\n\nclass BehaviorNode:\n    """Base class for behavior tree nodes"""\n    def __init__(self, name: str):\n        self.name = name\n        self.status = NodeStatus.RUNNING\n\n    def tick(self) -> NodeStatus:\n        """Execute one cycle of the node"""\n        raise NotImplementedError\n\nclass ActionNode(BehaviorNode):\n    """Base class for action nodes"""\n    def __init__(self, name: str, node: Node):\n        super().__init__(name)\n        self.node = node\n\n    def tick(self) -> NodeStatus:\n        """Execute the action"""\n        raise NotImplementedError\n\nclass SequenceNode(BehaviorNode):\n    """Sequence node executes children in order until one fails"""\n    def __init__(self, name: str):\n        super().__init__(name)\n        self.children = []\n        self.current_child_idx = 0\n\n    def add_child(self, child: BehaviorNode):\n        self.children.append(child)\n\n    def tick(self) -> NodeStatus:\n        while self.current_child_idx < len(self.children):\n            child_status = self.children[self.current_child_idx].tick()\n\n            if child_status == NodeStatus.FAILURE:\n                self.current_child_idx = 0\n                return NodeStatus.FAILURE\n            elif child_status == NodeStatus.RUNNING:\n                return NodeStatus.RUNNING\n            elif child_status == NodeStatus.SUCCESS:\n                self.current_child_idx += 1\n\n        # All children succeeded\n        self.current_child_idx = 0\n        return NodeStatus.SUCCESS\n\nclass SelectorNode(BehaviorNode):\n    """Selector node executes children until one succeeds"""\n    def __init__(self, name: str):\n        super().__init__(name)\n        self.children = []\n        self.current_child_idx = 0\n\n    def add_child(self, child: BehaviorNode):\n        self.children.append(child)\n\n    def tick(self) -> NodeStatus:\n        while self.current_child_idx < len(self.children):\n            child_status = self.children[self.current_child_idx].tick()\n\n            if child_status == NodeStatus.SUCCESS:\n                self.current_child_idx = 0\n                return NodeStatus.SUCCESS\n            elif child_status == NodeStatus.RUNNING:\n                return NodeStatus.RUNNING\n            elif child_status == NodeStatus.FAILURE:\n                self.current_child_idx += 1\n\n        # All children failed\n        self.current_child_idx = 0\n        return NodeStatus.FAILURE\n\nclass MoveToAction(ActionNode):\n    """Action to move robot to a specific pose"""\n    def __init__(self, name: str, node: Node, target_pose: Pose):\n        super().__init__(name, node)\n        self.target_pose = target_pose\n        self.nav_client = ActionClient(node, NavigateToPose, \'navigate_to_pose\')\n        self.goal_sent = False\n        self.goal_handle = None\n\n    def tick(self) -> NodeStatus:\n        if not self.goal_sent:\n            # Send navigation goal\n            goal_msg = NavigateToPose.Goal()\n            goal_msg.pose.header.frame_id = \'map\'\n            goal_msg.pose = self.target_pose\n\n            self.nav_client.wait_for_server()\n            future = self.nav_client.send_goal_async(goal_msg)\n            future.add_done_callback(self.goal_response_callback)\n            self.goal_sent = True\n            return NodeStatus.RUNNING\n\n        # Check if goal is still executing\n        if self.goal_handle is not None:\n            if self.goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\n                self.node.get_logger().info(f\'MoveToAction {self.name} succeeded\')\n                return NodeStatus.SUCCESS\n            elif self.goal_handle.status in [GoalStatus.STATUS_ABORTED, GoalStatus.STATUS_CANCELED]:\n                self.node.get_logger().info(f\'MoveToAction {self.name} failed\')\n                return NodeStatus.FAILURE\n            else:\n                return NodeStatus.RUNNING\n\n        return NodeStatus.RUNNING\n\n    def goal_response_callback(self, future):\n        self.goal_handle = future.result()\n\nclass DetectObjectAction(ActionNode):\n    """Action to detect a specific object"""\n    def __init__(self, name: str, node: Node, object_type: str):\n        super().__init__(name, node)\n        self.object_type = object_type\n        self.detected = False\n        self.detection_timeout = 10.0  # seconds\n        self.start_time = None\n\n    def tick(self) -> NodeStatus:\n        if self.start_time is None:\n            self.start_time = self.node.get_clock().now().nanoseconds / 1e9\n\n        # Check if object is detected (this would interface with perception system)\n        # For demo, we\'ll simulate detection\n        current_time = self.node.get_clock().now().nanoseconds / 1e9\n        elapsed = current_time - self.start_time\n\n        # Simulate detection after 2 seconds\n        if elapsed > 2.0:\n            self.detected = True\n            self.node.get_logger().info(f\'Detected {self.object_type}\')\n            return NodeStatus.SUCCESS\n\n        if elapsed > self.detection_timeout:\n            self.node.get_logger().info(f\'Timed out detecting {self.object_type}\')\n            return NodeStatus.FAILURE\n\n        return NodeStatus.RUNNING\n\nclass PickUpAction(ActionNode):\n    """Action to pick up an object"""\n    def __init__(self, name: str, node: Node):\n        super().__init__(name, node)\n        self.pick_success = False\n\n    def tick(self) -> NodeStatus:\n        # Simulate pick up action\n        # In real implementation, this would interface with manipulation stack\n        self.pick_success = True  # Simulate success\n        self.node.get_logger().info(\'Pick up action executed\')\n        return NodeStatus.SUCCESS if self.pick_success else NodeStatus.FAILURE\n\nclass PlaceDownAction(ActionNode):\n    """Action to place down an object"""\n    def __init__(self, name: str, node: Node):\n        super().__init__(name, node)\n        self.place_success = False\n\n    def tick(self) -> NodeStatus:\n        # Simulate place down action\n        self.place_success = True  # Simulate success\n        self.node.get_logger().info(\'Place down action executed\')\n        return NodeStatus.SUCCESS if self.place_success else NodeStatus.FAILURE\n\nclass ActionPlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'action_planner_node\')\n\n        # Publishers and subscribers\n        self.task_command_sub = self.create_subscription(\n            String, \'vla_task_command\', self.task_command_callback, 10)\n        self.status_pub = self.create_publisher(\n            String, \'action_status\', 10)\n\n        # Initialize behavior tree\n        self.behavior_tree = None\n\n        # Timer for executing behavior tree\n        self.tree_timer = self.create_timer(0.1, self.execute_behavior_tree)\n\n    def task_command_callback(self, msg):\n        """Handle task commands and build appropriate behavior tree"""\n        command = msg.data.lower()\n        self.get_logger().info(f\'Received task command: {command}\')\n\n        # Build behavior tree based on command\n        if \'fetch\' in command or \'bring\' in command:\n            self.build_fetch_behavior_tree(command)\n        elif \'navigate\' in command or \'go to\' in command:\n            self.build_navigation_behavior_tree(command)\n        else:\n            self.get_logger().warn(f\'Unknown task command: {command}\')\n            return\n\n    def build_fetch_behavior_tree(self, command):\n        """Build behavior tree for fetch tasks"""\n        # Example: "Go to kitchen, detect cup, pick up cup, bring to table"\n\n        # Create root selector (try different strategies)\n        root = SelectorNode(\'fetch_strategy_selector\')\n\n        # Create main sequence\n        main_sequence = SequenceNode(\'fetch_sequence\')\n\n        # Add subtasks to sequence\n        # 1. Navigate to source location (kitchen)\n        kitchen_pose = Pose()\n        kitchen_pose.position.x = 5.0\n        kitchen_pose.position.y = 3.0\n        kitchen_pose.orientation.w = 1.0\n\n        navigate_to_kitchen = MoveToAction(\'navigate_to_kitchen\', self, kitchen_pose)\n        main_sequence.add_child(navigate_to_kitchen)\n\n        # 2. Detect object (cup)\n        detect_cup = DetectObjectAction(\'detect_cup\', self, \'cup\')\n        main_sequence.add_child(detect_cup)\n\n        # 3. Pick up object\n        pick_up = PickUpAction(\'pick_up\', self)\n        main_sequence.add_child(pick_up)\n\n        # 4. Navigate to destination (table)\n        table_pose = Pose()\n        table_pose.position.x = 2.0\n        table_pose.position.y = 1.0\n        table_pose.orientation.w = 1.0\n\n        navigate_to_table = MoveToAction(\'navigate_to_table\', self, table_pose)\n        main_sequence.add_child(navigate_to_table)\n\n        # 5. Place down object\n        place_down = PlaceDownAction(\'place_down\', self)\n        main_sequence.add_child(place_down)\n\n        root.add_child(main_sequence)\n        self.behavior_tree = root\n\n        self.get_logger().info(\'Fetch behavior tree built\')\n\n    def build_navigation_behavior_tree(self, command):\n        """Build behavior tree for navigation tasks"""\n        # Create root sequence\n        root = SequenceNode(\'navigation_sequence\')\n\n        # Parse destination from command (simplified)\n        target_pose = Pose()\n        if \'kitchen\' in command:\n            target_pose.position.x = 5.0\n            target_pose.position.y = 3.0\n        elif \'living room\' in command:\n            target_pose.position.x = 1.0\n            target_pose.position.y = 2.0\n        else:\n            # Default to some location\n            target_pose.position.x = 0.0\n            target_pose.position.y = 0.0\n\n        target_pose.orientation.w = 1.0\n\n        # Add navigation action\n        navigate_action = MoveToAction(\'navigate_to_location\', self, target_pose)\n        root.add_child(navigate_action)\n\n        self.behavior_tree = root\n        self.get_logger().info(\'Navigation behavior tree built\')\n\n    def execute_behavior_tree(self):\n        """Execute the behavior tree"""\n        if self.behavior_tree is not None:\n            status = self.behavior_tree.tick()\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f\'Behavior tree status: {status.name}\'\n            self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    action_planner = ActionPlannerNode()\n\n    try:\n        rclpy.spin(action_planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        action_planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"state-machines-for-complex-actions",children:"State Machines for Complex Actions"}),"\n",(0,s.jsx)(n.p,{children:"State machines provide another approach to action execution:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="state_machine_action_executor.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nfrom enum import Enum\nfrom typing import Dict, Any\nimport time\n\nclass RobotState(Enum):\n    IDLE = 1\n    NAVIGATING = 2\n    DETECTING_OBJECT = 3\n    MANIPULATING = 4\n    RECHARGING = 5\n    EMERGENCY_STOP = 6\n\nclass ActionExecutorNode(Node):\n    def __init__(self):\n        super().__init__(\'action_executor_node\')\n\n        # Publishers and subscribers\n        self.task_command_sub = self.create_subscription(\n            String, \'state_task_command\', self.task_command_callback, 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'scan\', self.laser_callback, 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'state_status\', 10)\n\n        # Initialize state\n        self.current_state = RobotState.IDLE\n        self.target_pose = None\n        self.laser_data = None\n        self.battery_level = 100.0\n\n        # State timers\n        self.state_start_time = self.get_clock().now().nanoseconds / 1e9\n\n        # Timer for state machine execution\n        self.state_machine_timer = self.create_timer(0.1, self.execute_state_machine)\n\n    def task_command_callback(self, msg):\n        """Handle task commands"""\n        command = msg.data\n        self.get_logger().info(f\'Received state task: {command}\')\n\n        # Parse command and set target\n        if \'navigate\' in command.lower() or \'go to\' in command.lower():\n            # Extract coordinates from command (simplified)\n            self.current_state = RobotState.NAVIGATING\n            self.state_start_time = self.get_clock().now().nanoseconds / 1e9\n\n    def laser_callback(self, msg):\n        """Update laser data for obstacle detection"""\n        self.laser_data = msg.ranges\n\n    def execute_state_machine(self):\n        """Execute state machine logic"""\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        elapsed_time = current_time - self.state_start_time\n\n        # Check for emergency conditions\n        if self.should_enter_emergency_state():\n            self.current_state = RobotState.EMERGENCY_STOP\n            self.state_start_time = current_time\n\n        # Execute current state\n        if self.current_state == RobotState.IDLE:\n            self.execute_idle_state()\n        elif self.current_state == RobotState.NAVIGATING:\n            self.execute_navigating_state()\n        elif self.current_state == RobotState.DETECTING_OBJECT:\n            self.execute_detecting_object_state()\n        elif self.current_state == RobotState.MANIPULATING:\n            self.execute_manipulating_state()\n        elif self.current_state == RobotState.RECHARGING:\n            self.execute_recharging_state()\n        elif self.current_state == RobotState.EMERGENCY_STOP:\n            self.execute_emergency_stop_state()\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\'Current state: {self.current_state.name}, Battery: {self.battery_level}%\'\n        self.status_pub.publish(status_msg)\n\n    def should_enter_emergency_state(self) -> bool:\n        """Check if emergency state should be entered"""\n        if self.laser_data:\n            # Check for imminent collision (objects within 0.5m)\n            min_distance = min([r for r in self.laser_data if r > 0.0], default=float(\'inf\'))\n            if min_distance < 0.5:\n                return True\n\n        # Check battery level\n        if self.battery_level < 10.0:\n            return True\n\n        return False\n\n    def execute_idle_state(self):\n        """Execute idle state"""\n        # Stop robot\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.0\n        cmd_vel.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Check if new task arrived\n        # In real implementation, this would check for new commands\n\n    def execute_navigating_state(self):\n        """Execute navigation state"""\n        if self.laser_data:\n            # Simple obstacle avoidance\n            min_distance = min([r for r in self.laser_data if r > 0.0], default=float(\'inf\'))\n\n            cmd_vel = Twist()\n            if min_distance > 0.8:  # Safe distance\n                cmd_vel.linear.x = 0.3  # Move forward\n                cmd_vel.angular.z = 0.0\n            elif min_distance > 0.5:  # Need to turn\n                cmd_vel.linear.x = 0.1  # Slow down\n                cmd_vel.angular.z = 0.5  # Turn right\n            else:  # Emergency stop\n                cmd_vel.linear.x = 0.0\n                cmd_vel.angular.z = 0.0\n                self.current_state = RobotState.EMERGENCY_STOP\n\n            self.cmd_vel_pub.publish(cmd_vel)\n\n    def execute_detecting_object_state(self):\n        """Execute object detection state"""\n        # In real implementation, this would interface with perception system\n        # For demo, we\'ll just wait and then transition to next state\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        elapsed_time = current_time - self.state_start_time\n\n        if elapsed_time > 3.0:  # Detection timeout\n            self.current_state = RobotState.IDLE\n            self.state_start_time = current_time\n\n    def execute_manipulating_state(self):\n        """Execute manipulation state"""\n        # In real implementation, this would interface with manipulation stack\n        # For demo, we\'ll just wait and then transition to next state\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        elapsed_time = current_time - self.state_start_time\n\n        if elapsed_time > 2.0:  # Manipulation timeout\n            self.current_state = RobotState.IDLE\n            self.state_start_time = current_time\n\n    def execute_recharging_state(self):\n        """Execute recharging state"""\n        # In real implementation, navigate to charging station\n        # For demo, just reduce consumption\n        self.battery_level += 0.1  # Simulate charging\n        if self.battery_level >= 100.0:\n            self.battery_level = 100.0\n            self.current_state = RobotState.IDLE\n            self.state_start_time = self.get_clock().now().nanoseconds / 1e9\n\n    def execute_emergency_stop_state(self):\n        """Execute emergency stop state"""\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.0\n        cmd_vel.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Check if emergency condition is resolved\n        if self.laser_data:\n            min_distance = min([r for r in self.laser_data if r > 0.0], default=float(\'inf\'))\n            if min_distance > 0.8 and self.battery_level > 10.0:\n                self.current_state = RobotState.IDLE\n                self.state_start_time = self.get_clock().now().nanoseconds / 1e9\n\ndef main(args=None):\n    rclpy.init(args=args)\n    action_executor = ActionExecutorNode()\n\n    try:\n        rclpy.spin(action_executor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        action_executor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-vla-systems",children:"Integration with VLA Systems"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="vla_integration_node.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport json\nfrom typing import Dict, Any, Optional\n\nclass VLAIntegrationNode(Node):\n    def __init__(self):\n        super().__init__('vla_integration_node')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.vision_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.vision_callback, 10)\n        self.laser_sub = self.create_subscription(\n            LaserScan, 'scan', self.laser_callback, 10)\n        self.language_command_sub = self.create_subscription(\n            String, 'natural_language_command', self.language_callback, 10)\n        self.action_status_pub = self.create_publisher(\n            String, 'vla_action_status', 10)\n\n        # Action execution components\n        self.vision_data = None\n        self.perception_results = {}\n        self.pending_action = None\n        self.action_params = {}\n\n        # Timer for VLA execution\n        self.vla_timer = self.create_timer(0.1, self.vla_execution_loop)\n\n    def vision_callback(self, msg):\n        \"\"\"Process visual input\"\"\"\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.vision_data = cv_image\n\n            # Run perception pipeline\n            self.perception_results = self.run_perception_pipeline(cv_image)\n        except Exception as e:\n            self.get_logger().error(f'Error in vision processing: {e}')\n\n    def laser_callback(self, msg):\n        \"\"\"Process laser data for navigation context\"\"\"\n        self.laser_ranges = msg.ranges\n\n    def language_callback(self, msg):\n        \"\"\"Process natural language command\"\"\"\n        try:\n            # Parse command from language model\n            command_data = json.loads(msg.data)\n            action = command_data.get('action', 'unknown')\n            params = command_data.get('parameters', {})\n\n            self.pending_action = action\n            self.action_params = params\n\n            self.get_logger().info(f'Received VLA command: {action} with params {params}')\n        except json.JSONDecodeError:\n            # Handle plain text command\n            command_text = msg.data\n            self.get_logger().info(f'Received text command: {command_text}')\n            # In a real system, this would go through the language understanding pipeline\n\n    def run_perception_pipeline(self, image):\n        \"\"\"Run perception pipeline on image\"\"\"\n        # This would run object detection, segmentation, etc.\n        # For demo, return mock results\n        results = {\n            'objects': [\n                {'name': 'cup', 'confidence': 0.9, 'bbox': [100, 100, 200, 200]},\n                {'name': 'table', 'confidence': 0.85, 'bbox': [300, 300, 500, 400]}\n            ],\n            'room_type': 'kitchen',\n            'navigable_areas': ['center', 'left'],\n            'obstacles': []\n        }\n        return results\n\n    def vla_execution_loop(self):\n        \"\"\"Main VLA execution loop\"\"\"\n        if self.pending_action:\n            # Execute pending action with current context\n            success = self.execute_vla_action(self.pending_action, self.action_params)\n\n            if success:\n                self.get_logger().info(f'VLA action {self.pending_action} completed successfully')\n                self.pending_action = None\n                self.action_params = {}\n\n                # Publish success status\n                status_msg = String()\n                status_msg.data = f'SUCCESS: {self.pending_action}'\n                self.action_status_pub.publish(status_msg)\n            else:\n                self.get_logger().info(f'VLA action {self.pending_action} still executing')\n        else:\n            # Publish idle status\n            status_msg = String()\n            status_msg.data = 'IDLE: Waiting for VLA command'\n            self.action_status_pub.publish(status_msg)\n\n    def execute_vla_action(self, action: str, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute VLA action with current context\"\"\"\n        if action == 'navigate_to':\n            return self.execute_navigation_action(params)\n        elif action == 'detect_object':\n            return self.execute_detection_action(params)\n        elif action == 'pick_up':\n            return self.execute_pickup_action(params)\n        elif action == 'place_down':\n            return self.execute_placement_action(params)\n        elif action == 'describe_scene':\n            return self.execute_description_action(params)\n        else:\n            self.get_logger().warn(f'Unknown VLA action: {action}')\n            return True  # Mark as completed\n\n    def execute_navigation_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute navigation action with obstacle avoidance\"\"\"\n        target_x = params.get('x', 0.0)\n        target_y = params.get('y', 0.0)\n\n        # Check if we have laser data for navigation\n        if not hasattr(self, 'laser_ranges') or not self.laser_ranges:\n            return False\n\n        # Simple navigation with obstacle avoidance\n        cmd_vel = Twist()\n\n        # Check for obstacles in path\n        min_distance = min([r for r in self.laser_ranges if r > 0.0], default=float('inf'))\n\n        if min_distance < 0.5:\n            # Stop and wait for path to clear\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.0\n        else:\n            # Move toward target (simplified)\n            cmd_vel.linear.x = 0.2\n            cmd_vel.angular.z = 0.0\n\n        # Publish velocity command\n        cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        cmd_vel_pub.publish(cmd_vel)\n\n        # For demo, return True after some time to simulate completion\n        # In real system, check if target reached\n        return False  # Indicate still executing\n\n    def execute_detection_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute object detection action\"\"\"\n        target_object = params.get('object_type', 'unknown')\n\n        # Check if target object is in perception results\n        if 'objects' in self.perception_results:\n            for obj in self.perception_results['objects']:\n                if obj['name'] == target_object:\n                    self.get_logger().info(f'Found {target_object} with confidence {obj[\"confidence\"]}')\n                    return True\n\n        # Object not found, return False to continue searching\n        return False\n\n    def execute_pickup_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute pick up action\"\"\"\n        target_object = params.get('object', 'unknown')\n\n        # Check if object is in perception results and reachable\n        if 'objects' in self.perception_results:\n            for obj in self.perception_results['objects']:\n                if obj['name'] == target_object:\n                    # Simulate pickup action\n                    self.get_logger().info(f'Picking up {target_object}')\n                    return True\n\n        return False\n\n    def execute_placement_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute placement action\"\"\"\n        # Simulate placement action\n        self.get_logger().info('Placing object down')\n        return True\n\n    def execute_description_action(self, params: Dict[str, Any]) -> bool:\n        \"\"\"Execute scene description action\"\"\"\n        if self.perception_results:\n            description = f\"Scene contains: {[obj['name'] for obj in self.perception_results.get('objects', [])]}\"\n            self.get_logger().info(f'Scene description: {description}')\n\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_node = VLAIntegrationNode()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-action-planning",children:"Best Practices for Action Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modularity"}),": Design actions as modular, reusable components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling and recovery"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Include safety checks and emergency stop capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Continuously monitor execution status"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replanning"}),": Implement dynamic replanning for changing conditions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Action planning and execution complete the Vision-Language-Action pipeline by translating high-level commands into specific robot behaviors. Behavior trees and state machines provide structured approaches to managing complex action sequences. Proper integration of these components enables robots to perform sophisticated tasks based on natural language commands while maintaining safety and reliability."}),"\n",(0,s.jsx)(a.A,{title:"Action Planning Architecture",description:"Shows the components of action planning and execution in VLA systems"})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(f,{...e})}):f(e)}}}]);