"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[18],{6114:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>g,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>c,toc:()=>m});var t=a(4848),s=a(8453),o=a(1226),r=a(5783);const i={sidebar_position:3,title:"Language Models for Robot Control"},l="Language Models for Robot Control",c={id:"module-4-vla/topic-2",title:"Language Models for Robot Control",description:"Language models enable robots to understand and respond to natural language commands, bridging the gap between human communication and robotic action. This module covers the integration of large language models (LLMs) with robotic systems.",source:"@site/docs/module-4-vla/topic-2.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/topic-2",permalink:"/hackthon/docs/module-4-vla/topic-2",draft:!1,unlisted:!1,editUrl:"https://github.com/nehaltariq357/hackthon/tree/master/docs/module-4-vla/topic-2.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Language Models for Robot Control"},sidebar:"textbookSidebar",previous:{title:"Vision Systems for Robotics",permalink:"/hackthon/docs/module-4-vla/topic-1"},next:{title:"Action Planning and Execution",permalink:"/hackthon/docs/module-4-vla/topic-3"}},g={},m=[{value:"Overview of Language Model Integration",id:"overview-of-language-model-integration",level:2},{value:"Language Model Interface Node",id:"language-model-interface-node",level:2},{value:"Vision-Language Integration",id:"vision-language-integration",level:2},{value:"Task Planning with Language Models",id:"task-planning-with-language-models",level:2},{value:"Best Practices for Language Model Integration",id:"best-practices-for-language-model-integration",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.A,{}),"\n",(0,t.jsx)(n.h1,{id:"language-models-for-robot-control",children:"Language Models for Robot Control"}),"\n",(0,t.jsx)(n.p,{children:"Language models enable robots to understand and respond to natural language commands, bridging the gap between human communication and robotic action. This module covers the integration of large language models (LLMs) with robotic systems."}),"\n",(0,t.jsx)(n.h2,{id:"overview-of-language-model-integration",children:"Overview of Language Model Integration"}),"\n",(0,t.jsx)(n.p,{children:"Language models for robotics typically involve:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Parsing and interpreting commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Planning"}),": Converting language to executable actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Awareness"}),": Understanding the current environment and state"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Generation"}),": Creating specific robot commands from language"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"language-model-interface-node",children:"Language Model Interface Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="language_model_interface.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Twist\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport openai\nimport json\nimport re\nfrom typing import Dict, List, Any, Optional\n\nclass LanguageModelInterface(Node):\n    def __init__(self):\n        super().__init__('language_model_interface')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String, 'robot_command', self.command_callback, 10)\n        self.response_pub = self.create_publisher(\n            String, 'language_response', 10)\n        self.cmd_vel_pub = self.create_publisher(\n            Twist, 'cmd_vel', 10)\n\n        # Image subscription for context\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n\n        # Language model configuration\n        self.api_key = self.declare_parameter('openai_api_key', '').get_parameter_value().string_value\n        if self.api_key:\n            openai.api_key = self.api_key\n        else:\n            self.get_logger().warn('OpenAI API key not set, using mock responses')\n\n        # Robot state and context\n        self.current_image = None\n        self.robot_state = {\n            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'battery_level': 100.0,\n            'current_task': 'idle'\n        }\n\n        # Task execution queue\n        self.task_queue = []\n\n    def command_callback(self, msg):\n        \"\"\"Handle incoming natural language commands\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        try:\n            # Process command with language model\n            response = self.process_command_with_llm(command)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.response_pub.publish(response_msg)\n\n            # Execute tasks if any\n            self.execute_tasks_from_response(response)\n\n        except Exception as e:\n            error_msg = f'Error processing command: {e}'\n            self.get_logger().error(error_msg)\n            error_response = String()\n            error_response.data = error_msg\n            self.response_pub.publish(error_response)\n\n    def process_command_with_llm(self, command: str) -> str:\n        \"\"\"Process command using large language model\"\"\"\n        if not self.api_key:\n            # Mock response when API key is not available\n            return self.mock_language_processing(command)\n\n        # Prepare context for the language model\n        context = self.get_robot_context()\n\n        # Create prompt for the language model\n        prompt = f\"\"\"\n        You are a robot command interpreter. The robot has the following capabilities:\n        - Navigation: move_forward, turn_left, turn_right, stop\n        - Object manipulation: pick_up, place_down, grasp, release\n        - Sensors: camera, lidar, imu\n        - Current state: {context}\n\n        User command: \"{command}\"\n\n        Respond with a JSON object containing:\n        1. \"action\" - the primary action to take\n        2. \"parameters\" - any required parameters\n        3. \"explanation\" - brief explanation of the interpretation\n        4. \"confidence\" - confidence level (0-1)\n\n        Example response: {{\n            \"action\": \"navigate_to\",\n            \"parameters\": {{\"x\": 1.0, \"y\": 2.0}},\n            \"explanation\": \"Moving to specified coordinates\",\n            \"confidence\": 0.9\n        }}\n        \"\"\"\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=500\n            )\n\n            # Parse the response\n            content = response.choices[0].message.content\n            # Extract JSON from response (in case model includes additional text)\n            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n            if json_match:\n                parsed_response = json.loads(json_match.group())\n                return json.dumps(parsed_response)\n            else:\n                return json.dumps({\n                    \"action\": \"unknown\",\n                    \"parameters\": {},\n                    \"explanation\": \"Could not parse model response\",\n                    \"confidence\": 0.0\n                })\n\n        except Exception as e:\n            self.get_logger().error(f'LLM API error: {e}')\n            return self.mock_language_processing(command)\n\n    def mock_language_processing(self, command: str) -> str:\n        \"\"\"Mock language processing when API is not available\"\"\"\n        # Simple rule-based parsing for demonstration\n        command_lower = command.lower()\n\n        if 'move' in command_lower or 'go' in command_lower:\n            if 'forward' in command_lower:\n                action = 'move_forward'\n                params = {'distance': 1.0}\n            elif 'backward' in command_lower:\n                action = 'move_backward'\n                params = {'distance': 1.0}\n            elif 'left' in command_lower:\n                action = 'turn_left'\n                params = {'angle': 90}\n            elif 'right' in command_lower:\n                action = 'turn_right'\n                params = {'angle': 90}\n            else:\n                action = 'move_to'\n                params = {'x': 1.0, 'y': 1.0}\n        elif 'pick' in command_lower or 'grasp' in command_lower:\n            action = 'pick_up'\n            params = {'object': 'unknown'}\n        elif 'place' in command_lower or 'drop' in command_lower:\n            action = 'place_down'\n            params = {'object': 'unknown'}\n        else:\n            action = 'unknown'\n            params = {}\n\n        response = {\n            \"action\": action,\n            \"parameters\": params,\n            \"explanation\": f\"Interpreted command: {command}\",\n            \"confidence\": 0.8 if action != 'unknown' else 0.3\n        }\n\n        return json.dumps(response)\n\n    def get_robot_context(self) -> str:\n        \"\"\"Get current robot context for language model\"\"\"\n        context = {\n            'position': self.robot_state['position'],\n            'battery_level': self.robot_state['battery_level'],\n            'current_task': self.robot_state['current_task'],\n            'capabilities': ['navigation', 'manipulation', 'perception'],\n            'environment': 'indoor'  # This could come from perception system\n        }\n        return json.dumps(context)\n\n    def execute_tasks_from_response(self, response: str):\n        \"\"\"Execute tasks from language model response\"\"\"\n        try:\n            parsed_response = json.loads(response)\n            action = parsed_response.get('action', 'unknown')\n            params = parsed_response.get('parameters', {})\n\n            if action == 'move_forward':\n                self.execute_move_forward(params.get('distance', 1.0))\n            elif action == 'move_backward':\n                self.execute_move_forward(-params.get('distance', 1.0))\n            elif action == 'turn_left':\n                self.execute_turn(params.get('angle', 90))\n            elif action == 'turn_right':\n                self.execute_turn(-params.get('angle', 90))\n            elif action == 'move_to':\n                x = params.get('x', 0.0)\n                y = params.get('y', 0.0)\n                self.execute_move_to(x, y)\n            elif action == 'pick_up':\n                self.execute_pick_up()\n            elif action == 'place_down':\n                self.execute_place_down()\n            else:\n                self.get_logger().info(f'Unknown action: {action}')\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f'Error parsing response JSON: {e}')\n        except Exception as e:\n            self.get_logger().error(f'Error executing tasks: {e}')\n\n    def execute_move_forward(self, distance: float):\n        \"\"\"Execute move forward command\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.5  # Speed in m/s\n        cmd_vel.angular.z = 0.0\n\n        # Simple movement (in practice, use action clients for precise control)\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f'Moving forward {distance}m')\n\n    def execute_turn(self, angle_deg: float):\n        \"\"\"Execute turn command\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.0\n        cmd_vel.angular.z = 0.5  # Angular speed in rad/s\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f'Turning {angle_deg} degrees')\n\n    def execute_move_to(self, x: float, y: float):\n        \"\"\"Execute move to coordinates\"\"\"\n        # This would typically use navigation stack\n        self.get_logger().info(f'Moving to coordinates: ({x}, {y})')\n\n    def execute_pick_up(self):\n        \"\"\"Execute pick up object\"\"\"\n        self.get_logger().info('Attempting to pick up object')\n\n    def execute_place_down(self):\n        \"\"\"Execute place down object\"\"\"\n        self.get_logger().info('Attempting to place down object')\n\n    def image_callback(self, msg):\n        \"\"\"Store current image for context\"\"\"\n        try:\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        except Exception as e:\n            self.get_logger().warn(f'Error processing image: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    lang_model_node = LanguageModelInterface()\n\n    try:\n        rclpy.spin(lang_model_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        lang_model_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"vision-language-integration",children:"Vision-Language Integration"}),"\n",(0,t.jsx)(n.p,{children:"Language models become more powerful when combined with visual input:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="vision_language_integration.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport openai\nimport base64\nimport io\nfrom PIL import Image as PILImage\nimport json\nimport numpy as np\n\nclass VisionLanguageInterface(Node):\n    def __init__(self):\n        super().__init__(\'vision_language_interface\')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String, \'vision_language_command\', self.command_callback, 10)\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.response_pub = self.create_publisher(\n            String, \'vision_language_response\', 10)\n\n        # Language model configuration\n        self.api_key = self.declare_parameter(\'openai_api_key\', \'\').get_parameter_value().string_value\n        if self.api_key:\n            openai.api_key = self.api_key\n        else:\n            self.get_logger().warn(\'OpenAI API key not set, using mock responses\')\n\n        # Store latest image\n        self.latest_image = None\n        self.image_available = False\n\n    def command_callback(self, msg):\n        """Handle vision-language commands"""\n        command = msg.data\n        self.get_logger().info(f\'Received vision-language command: {command}\')\n\n        if not self.image_available:\n            response = "No image available for analysis"\n            response_msg = String()\n            response_msg.data = response\n            self.response_pub.publish(response_msg)\n            return\n\n        try:\n            # Process command with vision-language model\n            response = self.process_vision_language_command(command)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.response_pub.publish(response_msg)\n\n        except Exception as e:\n            error_msg = f\'Error in vision-language processing: {e}\'\n            self.get_logger().error(error_msg)\n            error_response = String()\n            error_response.data = error_msg\n            self.response_pub.publish(error_response)\n\n    def process_vision_language_command(self, command: str) -> str:\n        """Process command using vision-language model (GPT-4 Vision)"""\n        if not self.api_key:\n            # Mock response when API key is not available\n            return self.mock_vision_language_processing(command)\n\n        try:\n            # Convert OpenCV image to base64\n            _, buffer = cv2.imencode(\'.jpg\', self.latest_image)\n            image_base64 = base64.b64encode(buffer).decode(\'utf-8\')\n\n            # Create prompt for vision-language model\n            response = openai.ChatCompletion.create(\n                model="gpt-4-vision-preview",\n                messages=[\n                    {\n                        "role": "user",\n                        "content": [\n                            {"type": "text", "text": command},\n                            {\n                                "type": "image_url",\n                                "image_url": {\n                                    "url": f"data:image/jpeg;base64,{image_base64}"\n                                }\n                            }\n                        ]\n                    }\n                ],\n                max_tokens=500\n            )\n\n            return response.choices[0].message.content\n\n        except Exception as e:\n            self.get_logger().error(f\'Vision-language model error: {e}\')\n            return self.mock_vision_language_processing(command)\n\n    def mock_vision_language_processing(self, command: str) -> str:\n        """Mock vision-language processing"""\n        # This would analyze the image and command together\n        if \'red\' in command.lower():\n            return "I see a red object in the scene. It appears to be a cup on the left side of the image."\n        elif \'person\' in command.lower():\n            return "I see a person in the scene, approximately 2 meters away in front of the robot."\n        elif \'navigate\' in command.lower() or \'go\' in command.lower():\n            return "I can see a clear path forward. There is an obstacle to the right but the center path is clear."\n        else:\n            return f"I processed your command \'{command}\' with the current visual input. The scene contains various objects that I can identify and interact with."\n\n    def image_callback(self, msg):\n        """Handle incoming image"""\n        try:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.image_available = True\n            self.get_logger().debug(\'Received and stored new image for vision-language processing\')\n        except Exception as e:\n            self.get_logger().warn(f\'Error processing image: {e}\')\n            self.image_available = False\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_lang_node = VisionLanguageInterface()\n\n    try:\n        rclpy.spin(vision_lang_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_lang_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"task-planning-with-language-models",children:"Task Planning with Language Models"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="language_task_planner.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom geometry_msgs.msg import PoseStamped\nimport openai\nimport json\nimport re\nfrom typing import List, Dict, Any\n\nclass LanguageTaskPlanner(Node):\n    def __init__(self):\n        super().__init__(\'language_task_planner\')\n\n        # Publishers and subscribers\n        self.task_command_sub = self.create_subscription(\n            String, \'natural_task\', self.task_command_callback, 10)\n        self.status_pub = self.create_publisher(\n            String, \'task_status\', 10)\n\n        # Language model configuration\n        self.api_key = self.declare_parameter(\'openai_api_key\', \'\').get_parameter_value().string_value\n        if self.api_key:\n            openai.api_key = self.api_key\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Task queue\n        self.task_queue = []\n        self.current_task_index = 0\n\n    def task_command_callback(self, msg):\n        """Handle high-level task commands in natural language"""\n        task_description = msg.data\n        self.get_logger().info(f\'Received task: {task_description}\')\n\n        try:\n            # Parse the task using language model\n            task_plan = self.generate_task_plan(task_description)\n\n            # Execute the plan\n            self.execute_task_plan(task_plan)\n\n        except Exception as e:\n            error_msg = f\'Error planning task: {e}\'\n            self.get_logger().error(error_msg)\n            status_msg = String()\n            status_msg.data = error_msg\n            self.status_pub.publish(status_msg)\n\n    def generate_task_plan(self, task_description: str) -> List[Dict[str, Any]]:\n        """Generate a step-by-step task plan from natural language"""\n        if not self.api_key:\n            return self.mock_task_planning(task_description)\n\n        prompt = f"""\n        You are a robot task planner. Convert the following natural language task into a sequence of executable actions.\n        The robot has these capabilities:\n        - Navigate to location (with coordinates)\n        - Detect objects\n        - Manipulate objects (pick/place)\n        - Wait for conditions\n        - Communicate status\n\n        Task: "{task_description}"\n\n        Return a JSON array of actions, where each action has:\n        - "action": the action type (navigate, detect, manipulate, wait, communicate)\n        - "parameters": required parameters for the action\n        - "description": human-readable description\n\n        Example:\n        [\n            {{\n                "action": "navigate",\n                "parameters": {{"x": 1.0, "y": 2.0, "theta": 0.0}},\n                "description": "Move to kitchen area"\n            }},\n            {{\n                "action": "detect",\n                "parameters": {{"object_type": "cup"}},\n                "description": "Look for a cup"\n            }}\n        ]\n        """\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.2,\n                max_tokens=1000\n            )\n\n            content = response.choices[0].message.content\n            # Extract JSON from response\n            json_match = re.search(r\'\\[.*\\]\', content, re.DOTALL)\n            if json_match:\n                task_plan = json.loads(json_match.group())\n                return task_plan\n            else:\n                # If no JSON found, return empty plan\n                return []\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in task planning: {e}\')\n            return self.mock_task_planning(task_description)\n\n    def mock_task_planning(self, task_description: str) -> List[Dict[str, Any]]:\n        """Mock task planning when API is not available"""\n        # Simple rule-based planning for demonstration\n        task_plan = []\n\n        if \'kitchen\' in task_description.lower():\n            task_plan.append({\n                "action": "navigate",\n                "parameters": {"x": 5.0, "y": 3.0, "theta": 0.0},\n                "description": "Move to kitchen area"\n            })\n\n        if \'cup\' in task_description.lower() or \'object\' in task_description.lower():\n            task_plan.append({\n                "action": "detect",\n                "parameters": {"object_type": "cup"},\n                "description": "Look for a cup"\n            })\n            task_plan.append({\n                "action": "manipulate",\n                "parameters": {"action": "pick", "object": "cup"},\n                "description": "Pick up the cup"\n            })\n\n        if \'table\' in task_description.lower():\n            task_plan.append({\n                "action": "navigate",\n                "parameters": {"x": 2.0, "y": 1.0, "theta": 1.57},\n                "description": "Move to table"\n            })\n            task_plan.append({\n                "action": "manipulate",\n                "parameters": {"action": "place", "object": "cup"},\n                "description": "Place the cup on table"\n            })\n\n        return task_plan\n\n    def execute_task_plan(self, task_plan: List[Dict[str, Any]]):\n        """Execute the generated task plan"""\n        self.task_queue = task_plan\n        self.current_task_index = 0\n\n        if self.task_queue:\n            self.execute_next_task()\n        else:\n            status_msg = String()\n            status_msg.data = "No tasks to execute"\n            self.status_pub.publish(status_msg)\n\n    def execute_next_task(self):\n        """Execute the next task in the queue"""\n        if self.current_task_index >= len(self.task_queue):\n            # All tasks completed\n            status_msg = String()\n            status_msg.data = "Task plan completed successfully"\n            self.status_pub.publish(status_msg)\n            return\n\n        current_task = self.task_queue[self.current_task_index]\n        self.get_logger().info(f\'Executing task: {current_task["description"]}\')\n\n        # Update status\n        status_msg = String()\n        status_msg.data = f\'Executing: {current_task["description"]}\'\n        self.status_pub.publish(status_msg)\n\n        # Execute based on action type\n        action_type = current_task["action"]\n        if action_type == "navigate":\n            self.execute_navigation_task(current_task["parameters"])\n        elif action_type == "detect":\n            self.execute_detection_task(current_task["parameters"])\n        elif action_type == "manipulate":\n            self.execute_manipulation_task(current_task["parameters"])\n        elif action_type == "communicate":\n            self.execute_communication_task(current_task["parameters"])\n\n    def execute_navigation_task(self, params: Dict[str, Any]):\n        """Execute navigation task"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.pose.position.x = float(params.get(\'x\', 0.0))\n        goal_msg.pose.pose.position.y = float(params.get(\'y\', 0.0))\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Simple orientation (facing forward)\n        from math import sin, cos\n        theta = params.get(\'theta\', 0.0)\n        goal_msg.pose.pose.orientation.z = sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = cos(theta / 2.0)\n\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_done_callback)\n\n    def navigation_done_callback(self, future):\n        """Handle completion of navigation task"""\n        goal_result = future.result()\n        if goal_result.status == GoalStatus.STATUS_SUCCEEDED:\n            self.get_logger().info(\'Navigation task completed successfully\')\n        else:\n            self.get_logger().warn(\'Navigation task failed\')\n\n        # Move to next task\n        self.current_task_index += 1\n        self.execute_next_task()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    task_planner = LanguageTaskPlanner()\n\n    try:\n        rclpy.spin(task_planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        task_planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices-for-language-model-integration",children:"Best Practices for Language Model Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Always validate and confirm critical commands before execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context"}),": Provide sufficient context to the language model for accurate interpretation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fallback"}),": Implement fallback mechanisms when language models fail"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Privacy"}),": Consider privacy implications when using cloud-based language models"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),": Optimize for real-time performance in interactive scenarios"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Language models enable natural interaction with robotic systems, allowing users to communicate with robots using everyday language. By combining language understanding with vision and action planning, robots can perform complex tasks based on high-level instructions. Proper integration requires careful consideration of safety, context, and real-time performance requirements."}),"\n",(0,t.jsx)(o.A,{title:"Vision-Language-Action Integration",description:"Shows how language models connect perception and action in robotic systems"})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);