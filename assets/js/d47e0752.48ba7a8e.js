"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[689],{9533:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>m,contentTitle:()=>l,default:()=>f,frontMatter:()=>a,metadata:()=>c,toc:()=>d});var i=s(4848),t=s(8453),o=s(1226),r=s(5783);const a={sidebar_position:2,title:"Vision Systems for Robotics"},l="Vision Systems for Robotics",c={id:"module-4-vla/topic-1",title:"Vision Systems for Robotics",description:"Vision systems form the eyes of robotic systems, enabling them to perceive and understand their environment. In the context of Vision-Language-Action (VLA) models, vision systems provide the crucial input for decision-making and action execution.",source:"@site/docs/module-4-vla/topic-1.mdx",sourceDirName:"module-4-vla",slug:"/module-4-vla/topic-1",permalink:"/hackthon/docs/module-4-vla/topic-1",draft:!1,unlisted:!1,editUrl:"https://github.com/nehaltariq357/hackthon/tree/master/docs/module-4-vla/topic-1.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Vision Systems for Robotics"},sidebar:"textbookSidebar",previous:{title:"Module 4: Vision-Language-Action (VLA)",permalink:"/hackthon/docs/module-4-vla/"},next:{title:"Language Models for Robot Control",permalink:"/hackthon/docs/module-4-vla/topic-2"}},m={},d=[{value:"Overview of Robotic Vision Systems",id:"overview-of-robotic-vision-systems",level:2},{value:"Vision Processing Pipeline",id:"vision-processing-pipeline",level:2},{value:"Advanced Vision Techniques",id:"advanced-vision-techniques",level:2},{value:"Stereo Vision for Depth Perception",id:"stereo-vision-for-depth-perception",level:3},{value:"Visual SLAM Integration",id:"visual-slam-integration",level:2},{value:"Best Practices for Vision Systems",id:"best-practices-for-vision-systems",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.A,{}),"\n",(0,i.jsx)(n.h1,{id:"vision-systems-for-robotics",children:"Vision Systems for Robotics"}),"\n",(0,i.jsx)(n.p,{children:"Vision systems form the eyes of robotic systems, enabling them to perceive and understand their environment. In the context of Vision-Language-Action (VLA) models, vision systems provide the crucial input for decision-making and action execution."}),"\n",(0,i.jsx)(n.h2,{id:"overview-of-robotic-vision-systems",children:"Overview of Robotic Vision Systems"}),"\n",(0,i.jsx)(n.p,{children:"Robotic vision systems encompass various technologies and algorithms:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Camera Systems"}),": RGB, stereo, thermal, and specialized cameras"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Image Processing"}),": Filtering, enhancement, and preprocessing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feature Detection"}),": Keypoints, edges, and object detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"3D Vision"}),": Depth estimation, stereo vision, and SLAM"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deep Learning"}),": CNNs, transformers, and specialized architectures"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"vision-processing-pipeline",children:"Vision Processing Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"Here's an example of a comprehensive vision processing pipeline for robotics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="robotic_vision_pipeline.py"',children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom typing import List, Tuple, Optional\n\nclass RoboticVisionPipeline(Node):\n    def __init__(self):\n        super().__init__('robotic_vision_pipeline')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, 'camera/camera_info', self.camera_info_callback, 10)\n\n        self.detections_pub = self.create_publisher(\n            Detection2DArray, 'vision/detections', 10)\n        self.depth_pub = self.create_publisher(\n            Image, 'vision/depth_map', 10)\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Initialize vision models\n        self.initialize_models()\n\n        # Transformation utilities\n        self.transform = T.Compose([\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def initialize_models(self):\n        \"\"\"Initialize vision models for perception\"\"\"\n        # Load pre-trained object detection model\n        # In practice, this would load models like YOLO, Detectron2, etc.\n        try:\n            # Example using TorchVision\n            self.detection_model = torch.hub.load(\n                'ultralytics/yolov5', 'yolov5s', pretrained=True\n            )\n            self.detection_model.eval()\n            self.get_logger().info('Object detection model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load detection model: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Update camera parameters from camera info\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.distortion_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Preprocess image\n            processed_image = self.preprocess_image(cv_image)\n\n            # Run object detection\n            detections = self.run_object_detection(processed_image)\n\n            # Estimate depth if stereo camera is available\n            depth_map = self.estimate_depth(processed_image)\n\n            # Publish results\n            self.publish_detections(detections, msg.header)\n            if depth_map is not None:\n                depth_msg = self.bridge.cv2_to_imgmsg(depth_map, encoding='32FC1')\n                depth_msg.header = msg.header\n                self.depth_pub.publish(depth_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess image for vision pipeline\"\"\"\n        # Undistort image if camera parameters are available\n        if self.camera_matrix is not None and self.distortion_coeffs is not None:\n            image = cv2.undistort(\n                image, self.camera_matrix, self.distortion_coeffs\n            )\n\n        # Resize image to standard size for model input\n        height, width = image.shape[:2]\n        if height != 416 or width != 416:\n            image = cv2.resize(image, (416, 416))\n\n        return image\n\n    def run_object_detection(self, image: np.ndarray) -> List[dict]:\n        \"\"\"Run object detection on image\"\"\"\n        try:\n            # Convert image for model input\n            img_tensor = self.transform(image).unsqueeze(0)\n\n            # Run detection\n            with torch.no_grad():\n                results = self.detection_model(img_tensor)\n\n            # Process results\n            detections = []\n            for *xyxy, conf, cls in results.xyxy[0].tolist():\n                if conf > 0.5:  # Confidence threshold\n                    detection = {\n                        'bbox': [int(coord) for coord in xyxy],\n                        'confidence': conf,\n                        'class_id': int(cls),\n                        'class_name': self.get_class_name(int(cls))\n                    }\n                    detections.append(detection)\n\n            return detections\n        except Exception as e:\n            self.get_logger().error(f'Error in object detection: {e}')\n            return []\n\n    def estimate_depth(self, image: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Estimate depth from single image (monocular) or stereo\"\"\"\n        # For stereo vision, this would use stereo matching\n        # For monocular, this would use a depth estimation model\n        try:\n            # Placeholder for depth estimation\n            # In practice, use models like MiDaS, DeepV2D, etc.\n            height, width = image.shape[:2]\n            depth_map = np.random.rand(height, width).astype(np.float32) * 10.0  # Placeholder\n\n            return depth_map\n        except Exception as e:\n            self.get_logger().warn(f'Depth estimation failed: {e}')\n            return None\n\n    def get_class_name(self, class_id: int) -> str:\n        \"\"\"Get class name from class ID\"\"\"\n        # COCO dataset class names\n        coco_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        if 0 <= class_id < len(coco_names):\n            return coco_names[class_id]\n        else:\n            return f'unknown_{class_id}'\n\n    def publish_detections(self, detections: List[dict], header):\n        \"\"\"Publish detections as ROS message\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        for detection in detections:\n            detection_msg = Detection2D()\n            detection_msg.header = header\n\n            # Bounding box\n            bbox = BoundingBox2D()\n            bbox.center.position.x = (detection['bbox'][0] + detection['bbox'][2]) / 2\n            bbox.center.position.y = (detection['bbox'][1] + detection['bbox'][3]) / 2\n            bbox.size_x = detection['bbox'][2] - detection['bbox'][0]\n            bbox.size_y = detection['bbox'][3] - detection['bbox'][1]\n            detection_msg.bbox = bbox\n\n            # Results\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = detection['class_name']\n            result.hypothesis.score = detection['confidence']\n            detection_msg.results.append(result)\n\n            detections_msg.detections.append(detection_msg)\n\n        self.detections_pub.publish(detections_msg)\n        self.get_logger().info(f'Published {len(detections)} detections')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_pipeline = RoboticVisionPipeline()\n\n    try:\n        rclpy.spin(vision_pipeline)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"advanced-vision-techniques",children:"Advanced Vision Techniques"}),"\n",(0,i.jsx)(n.h3,{id:"stereo-vision-for-depth-perception",children:"Stereo Vision for Depth Perception"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="stereo_vision_node.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom sensor_msgs.msg import CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass StereoVisionNode(Node):\n    def __init__(self):\n        super().__init__(\'stereo_vision_node\')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Subscribers for stereo pair\n        self.left_sub = self.create_subscription(\n            Image, \'stereo/left/image_rect\', self.left_callback, 10)\n        self.right_sub = self.create_subscription(\n            Image, \'stereo/right/image_rect\', self.right_callback, 10)\n\n        # Camera info subscribers\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, \'stereo/left/camera_info\', self.left_info_callback, 10)\n        self.right_info_sub = self.create_subscription(\n            CameraInfo, \'stereo/right/camera_info\', self.right_info_callback, 10)\n\n        # Publisher for disparity map\n        self.disparity_pub = self.create_publisher(Image, \'stereo/disparity\', 10)\n\n        # Stereo matcher\n        self.stereo_matcher = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=128,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Camera parameters\n        self.left_camera_matrix = None\n        self.right_camera_matrix = None\n        self.rotation_matrix = None\n        self.translation_vector = None\n\n        # Store latest images\n        self.latest_left = None\n        self.latest_right = None\n\n    def left_info_callback(self, msg):\n        """Handle left camera info"""\n        self.left_camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def right_info_callback(self, msg):\n        """Handle right camera info"""\n        self.right_camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def left_callback(self, msg):\n        """Handle left camera image"""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n        self.latest_left = cv_image\n\n        # Process stereo pair if both images are available\n        if self.latest_right is not None:\n            self.process_stereo_pair()\n\n    def right_callback(self, msg):\n        """Handle right camera image"""\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n        self.latest_right = cv_image\n\n        # Process stereo pair if both images are available\n        if self.latest_left is not None:\n            self.process_stereo_pair()\n\n    def process_stereo_pair(self):\n        """Process stereo image pair to generate disparity map"""\n        if self.latest_left is None or self.latest_right is None:\n            return\n\n        # Compute disparity\n        disparity = self.stereo_matcher.compute(\n            self.latest_left, self.latest_right\n        ).astype(np.float32)\n\n        # Convert to depth map\n        disparity = disparity / 16.0  # SGBM returns 16x disparity values\n\n        # Publish disparity map\n        disparity_msg = self.bridge.cv2_to_imgmsg(disparity, encoding=\'32FC1\')\n        disparity_msg.header.stamp = self.get_clock().now().to_msg()\n        disparity_msg.header.frame_id = \'stereo_link\'\n        self.disparity_pub.publish(disparity_msg)\n\n        self.get_logger().info(\'Published disparity map\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    stereo_node = StereoVisionNode()\n\n    try:\n        rclpy.spin(stereo_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        stereo_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"visual-slam-integration",children:"Visual SLAM Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="visual_slam_node.py"',children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import PoseStamped, TransformStamped\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\n\nclass VisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'visual_slam_node\')\n\n        # Initialize CV Bridge\n        self.bridge = CvBridge()\n\n        # Publisher for robot pose\n        self.pose_pub = self.create_publisher(PoseStamped, \'visual_slam/pose\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n\n        # Transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # ORB feature detector\n        self.orb = cv2.ORB_create(nfeatures=1000)\n        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n        # SLAM state\n        self.previous_keypoints = None\n        self.previous_descriptors = None\n        self.current_position = np.array([0.0, 0.0, 0.0])\n        self.current_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion\n\n        # Previous image for motion estimation\n        self.previous_image = None\n\n    def image_callback(self, msg):\n        """Process incoming image for SLAM"""\n        try:\n            # Convert ROS image to OpenCV\n            current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'mono8\')\n\n            if self.previous_image is not None:\n                # Extract features\n                current_keypoints, current_descriptors = self.extract_features(current_image)\n\n                if self.previous_keypoints is not None and current_keypoints is not None:\n                    # Match features\n                    matches = self.match_features(\n                        self.previous_descriptors, current_descriptors\n                    )\n\n                    if len(matches) >= 10:  # Minimum matches for reliable pose estimation\n                        # Estimate motion\n                        motion = self.estimate_motion(\n                            self.previous_keypoints, current_keypoints, matches\n                        )\n\n                        if motion is not None:\n                            # Update position\n                            self.update_position(motion)\n\n                            # Publish pose\n                            self.publish_pose(msg.header)\n\n            # Store current data for next iteration\n            self.previous_image = current_image\n            self.previous_keypoints = self.extract_features(current_image)[0]\n            self.previous_descriptors = self.extract_features(current_image)[1]\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in SLAM processing: {e}\')\n\n    def extract_features(self, image):\n        """Extract ORB features from image"""\n        keypoints = self.orb.detect(image, None)\n        keypoints, descriptors = self.orb.compute(image, keypoints)\n        return keypoints, descriptors\n\n    def match_features(self, desc1, desc2):\n        """Match features between two images"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        try:\n            matches = self.bf.match(desc1, desc2)\n            matches = sorted(matches, key=lambda x: x.distance)\n            return matches\n        except Exception as e:\n            self.get_logger().warn(f\'Feature matching failed: {e}\')\n            return []\n\n    def estimate_motion(self, kp1, kp2, matches):\n        """Estimate motion between two frames"""\n        if len(matches) < 10:\n            return None\n\n        # Get matched keypoints\n        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Compute homography\n        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\n        if homography is not None:\n            # Extract translation from homography (simplified)\n            dx = homography[0, 2]\n            dy = homography[1, 2]\n\n            # Convert to reasonable scale (this is simplified)\n            translation = np.array([dx * 0.01, dy * 0.01, 0.0])  # Scale factor is arbitrary\n            return translation\n\n        return None\n\n    def update_position(self, motion):\n        """Update robot position based on estimated motion"""\n        self.current_position += motion\n\n    def publish_pose(self, header):\n        """Publish current pose and broadcast transform"""\n        # Create pose message\n        pose_msg = PoseStamped()\n        pose_msg.header = header\n        pose_msg.pose.position.x = float(self.current_position[0])\n        pose_msg.pose.position.y = float(self.current_position[1])\n        pose_msg.pose.position.z = float(self.current_position[2])\n        pose_msg.pose.orientation.x = float(self.current_orientation[0])\n        pose_msg.pose.orientation.y = float(self.current_orientation[1])\n        pose_msg.pose.orientation.z = float(self.current_orientation[2])\n        pose_msg.pose.orientation.w = float(self.current_orientation[3])\n\n        self.pose_pub.publish(pose_msg)\n\n        # Broadcast transform\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = \'map\'\n        t.child_frame_id = \'visual_slam_frame\'\n        t.transform.translation.x = float(self.current_position[0])\n        t.transform.translation.y = float(self.current_position[1])\n        t.transform.translation.z = float(self.current_position[2])\n        t.transform.rotation.x = float(self.current_orientation[0])\n        t.transform.rotation.y = float(self.current_orientation[1])\n        t.transform.rotation.z = float(self.current_orientation[2])\n        t.transform.rotation.w = float(self.current_orientation[3])\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    slam_node = VisualSLAMNode()\n\n    try:\n        rclpy.spin(slam_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        slam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-vision-systems",children:"Best Practices for Vision Systems"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lighting Conditions"}),": Account for varying lighting in algorithm design"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Optimize algorithms for real-time performance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),": Regularly calibrate cameras and vision systems"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Design systems to handle partial occlusions and noise"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validation"}),": Test vision systems under various conditions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Vision systems provide robotic systems with the ability to perceive and understand their environment. By implementing robust vision processing pipelines, feature detection, and depth estimation, robots can navigate and interact with the world effectively. These capabilities form the foundation for higher-level Vision-Language-Action systems."}),"\n",(0,i.jsx)(o.A,{title:"Robotic Vision System Architecture",description:"Shows the components of a robotic vision system from cameras to perception outputs"})]})}function f(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);