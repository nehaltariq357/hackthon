"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[601],{4286:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>a,toc:()=>d});var o=n(4848),t=n(8453);const l={sidebar_position:4,title:"Module 4: Vision-Language-Action (VLA)"},s="Module 4: Vision-Language-Action (VLA)",a={id:"module-4-vla/index",title:"Module 4: Vision-Language-Action (VLA)",description:"This module explores Vision-Language-Action (VLA) models, which represent the next generation of robotic intelligence. You'll learn how to integrate perception, reasoning, and action in a unified framework.",source:"@site/docs/module-4-vla/index.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/",permalink:"/docs/module-4-vla/",draft:!1,unlisted:!1,editUrl:"https://github.com/nehaltariq357/hackthon/tree/master/docs/module-4-vla/index.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,title:"Module 4: Vision-Language-Action (VLA)"},sidebar:"textbookSidebar",previous:{title:"Simulation and Deployment",permalink:"/docs/module-3-nvidia-isaac/topic-3"},next:{title:"Vision Systems for Robotics",permalink:"/docs/module-4-vla/topic-1"}},r={},d=[{value:"Overview",id:"overview",level:2},{value:"Topics in this Module",id:"topics-in-this-module",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2}];function c(e){const i={a:"a",h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,o.jsx)(i.p,{children:"This module explores Vision-Language-Action (VLA) models, which represent the next generation of robotic intelligence. You'll learn how to integrate perception, reasoning, and action in a unified framework."}),"\n",(0,o.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(i.p,{children:"Vision-Language-Action (VLA) models represent a paradigm shift in robotics, where vision, language understanding, and action planning are integrated into unified models. These models enable robots to understand natural language commands and execute complex tasks in real-world environments."}),"\n",(0,o.jsx)(i.h2,{id:"topics-in-this-module",children:"Topics in this Module"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/docs/module-4-vla/topic-1",children:"Vision Systems for Robotics"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/docs/module-4-vla/topic-2",children:"Language Models for Robot Control"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/docs/module-4-vla/topic-3",children:"Action Planning and Execution"})}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(i.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Implement vision systems for robotic perception"}),"\n",(0,o.jsx)(i.li,{children:"Integrate language models with robotic control"}),"\n",(0,o.jsx)(i.li,{children:"Design action planning algorithms"}),"\n",(0,o.jsx)(i.li,{children:"Create robots that respond to natural language commands"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Basic knowledge of ROS 2 (from Module 1)"}),"\n",(0,o.jsx)(i.li,{children:"Understanding of computer vision concepts"}),"\n",(0,o.jsx)(i.li,{children:"Familiarity with natural language processing"}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,o.jsx)(i,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);