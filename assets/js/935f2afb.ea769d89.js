"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"textbookSidebar":[{"type":"category","label":"Module 1: ROS 2 \u2013 Robotic Nervous System","items":[{"type":"link","label":"Introduction to ROS 2 Architecture","href":"/docs/module-1-ros2/topic-1","docId":"module-1-ros2/topic-1","unlisted":false},{"type":"link","label":"ROS 2 Nodes and Topics","href":"/docs/module-1-ros2/topic-2","docId":"module-1-ros2/topic-2","unlisted":false},{"type":"link","label":"ROS 2 Services and Actions","href":"/docs/module-1-ros2/topic-3","docId":"module-1-ros2/topic-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/module-1-ros2/"},{"type":"category","label":"Module 2: Digital Twin \u2013 Gazebo & Unity","items":[{"type":"link","label":"Gazebo Simulation Environment","href":"/docs/module-2-digital-twin/topic-1","docId":"module-2-digital-twin/topic-1","unlisted":false},{"type":"link","label":"Unity Integration for Robotics","href":"/docs/module-2-digital-twin/topic-2","docId":"module-2-digital-twin/topic-2","unlisted":false},{"type":"link","label":"Digital Twin Concepts and Applications","href":"/docs/module-2-digital-twin/topic-3","docId":"module-2-digital-twin/topic-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/module-2-digital-twin/"},{"type":"category","label":"Module 3: NVIDIA Isaac AI Brain","items":[{"type":"link","label":"Isaac ROS Framework","href":"/docs/module-3-nvidia-isaac/topic-1","docId":"module-3-nvidia-isaac/topic-1","unlisted":false},{"type":"link","label":"AI Tools and Acceleration","href":"/docs/module-3-nvidia-isaac/topic-2","docId":"module-3-nvidia-isaac/topic-2","unlisted":false},{"type":"link","label":"Simulation and Deployment","href":"/docs/module-3-nvidia-isaac/topic-3","docId":"module-3-nvidia-isaac/topic-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/module-3-nvidia-isaac/"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","label":"Vision Systems for Robotics","href":"/docs/module-4-vla/topic-1","docId":"module-4-vla/topic-1","unlisted":false},{"type":"link","label":"Language Models for Robot Control","href":"/docs/module-4-vla/topic-2","docId":"module-4-vla/topic-2","unlisted":false},{"type":"link","label":"Action Planning and Execution","href":"/docs/module-4-vla/topic-3","docId":"module-4-vla/topic-3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/module-4-vla/"},{"type":"category","label":"Capstone Project: Autonomous Humanoid Robot","items":[{"type":"link","label":"Humanoid Robot Integration","href":"/docs/capstone-project/topic-1","docId":"capstone-project/topic-1","unlisted":false},{"type":"link","label":"Autonomous Behavior Implementation","href":"/docs/capstone-project/topic-2","docId":"capstone-project/topic-2","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/docs/capstone-project/"}]},"docs":{"capstone-project/index":{"id":"capstone-project/index","title":"Capstone Project: Autonomous Humanoid Robot","description":"The capstone project integrates all concepts learned in the previous modules to design and implement an autonomous humanoid robot. This project demonstrates the application of ROS 2, digital twins, AI, and VLA models in a complex robotic system.","sidebar":"textbookSidebar"},"capstone-project/topic-1":{"id":"capstone-project/topic-1","title":"Humanoid Robot Integration","description":"The capstone project integrates all concepts from previous modules into a complete autonomous humanoid robot system. This module focuses on the integration aspects of combining vision, language understanding, and action execution in a humanoid form factor.","sidebar":"textbookSidebar"},"capstone-project/topic-2":{"id":"capstone-project/topic-2","title":"Autonomous Behavior Implementation","description":"This module focuses on implementing autonomous behaviors that integrate all components of the humanoid robot system. We\'ll create complex behaviors that demonstrate the full capability of the Vision-Language-Action framework in a humanoid context.","sidebar":"textbookSidebar"},"module-1-ros2/index":{"id":"module-1-ros2/index","title":"Module 1: ROS 2 \u2013 Robotic Nervous System","description":"Welcome to the first module of our AI Robotics Textbook. In this module, you\'ll learn about ROS 2 (Robot Operating System 2), which serves as the nervous system for robotic applications.","sidebar":"textbookSidebar"},"module-1-ros2/topic-1":{"id":"module-1-ros2/topic-1","title":"Introduction to ROS 2 Architecture","description":"ROS 2 (Robot Operating System 2) provides a flexible framework for writing robot software. It\'s designed to support the development of complex robotic applications with features like distributed computing, real-time support, and deployment in embedded systems.","sidebar":"textbookSidebar"},"module-1-ros2/topic-2":{"id":"module-1-ros2/topic-2","title":"ROS 2 Nodes and Topics","description":"In ROS 2, nodes are the fundamental building blocks of a robotic application. Nodes communicate with each other using topics, which implement a publish-subscribe communication pattern. Understanding nodes and topics is essential for building distributed robotic systems.","sidebar":"textbookSidebar"},"module-1-ros2/topic-3":{"id":"module-1-ros2/topic-3","title":"ROS 2 Services and Actions","description":"While topics provide asynchronous publish-subscribe communication, services and actions provide synchronous request-response and goal-oriented communication patterns respectively. These are essential for implementing blocking operations and complex task management in robotic systems.","sidebar":"textbookSidebar"},"module-2-digital-twin/index":{"id":"module-2-digital-twin/index","title":"Module 2: Digital Twin \u2013 Gazebo & Unity","description":"In this module, you\'ll explore digital twin technologies for robotics, focusing on Gazebo and Unity simulation environments. Digital twins are virtual replicas of physical systems that enable testing and validation before deployment.","sidebar":"textbookSidebar"},"module-2-digital-twin/topic-1":{"id":"module-2-digital-twin/topic-1","title":"Gazebo Simulation Environment","description":"Gazebo is a powerful 3D simulation environment for robotics that provides realistic physics simulation, high-quality graphics, and convenient programmatic interfaces. It\'s widely used in robotics research and development.","sidebar":"textbookSidebar"},"module-2-digital-twin/topic-2":{"id":"module-2-digital-twin/topic-2","title":"Unity Integration for Robotics","description":"Unity is a powerful 3D development platform that can be used for robotics simulation and visualization. Its advanced rendering capabilities and physics engine make it suitable for creating high-fidelity digital twins of robotic systems.","sidebar":"textbookSidebar"},"module-2-digital-twin/topic-3":{"id":"module-2-digital-twin/topic-3","title":"Digital Twin Concepts and Applications","description":"Digital twins are virtual replicas of physical systems that enable real-time monitoring, simulation, and optimization. In robotics, digital twins play a crucial role in development, testing, and deployment of robotic systems.","sidebar":"textbookSidebar"},"module-3-nvidia-isaac/index":{"id":"module-3-nvidia-isaac/index","title":"Module 3: NVIDIA Isaac AI Brain","description":"This module focuses on NVIDIA Isaac, a comprehensive robotics platform that combines hardware and software to provide AI capabilities for robots. You\'ll learn how to leverage NVIDIA\'s GPU-accelerated computing for robotics applications.","sidebar":"textbookSidebar"},"module-3-nvidia-isaac/topic-1":{"id":"module-3-nvidia-isaac/topic-1","title":"Isaac ROS Framework","description":"Isaac ROS is NVIDIA\'s robotics platform that brings GPU acceleration and AI capabilities to ROS 2. It provides a collection of hardware-accelerated packages that enable robots to perceive, understand, and navigate the world around them.","sidebar":"textbookSidebar"},"module-3-nvidia-isaac/topic-2":{"id":"module-3-nvidia-isaac/topic-2","title":"AI Tools and Acceleration","description":"NVIDIA Isaac provides powerful AI tools and acceleration capabilities that enable robots to perform complex perception, reasoning, and control tasks. This module covers the AI frameworks and tools available in the Isaac ecosystem.","sidebar":"textbookSidebar"},"module-3-nvidia-isaac/topic-3":{"id":"module-3-nvidia-isaac/topic-3","title":"Simulation and Deployment","description":"This module covers the simulation and deployment aspects of NVIDIA Isaac, including Isaac Sim for advanced simulation and deployment strategies for Isaac-powered robotic systems.","sidebar":"textbookSidebar"},"module-4-vla/index":{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"This module explores Vision-Language-Action (VLA) models, which represent the next generation of robotic intelligence. You\'ll learn how to integrate perception, reasoning, and action in a unified framework.","sidebar":"textbookSidebar"},"module-4-vla/topic-1":{"id":"module-4-vla/topic-1","title":"Vision Systems for Robotics","description":"Vision systems form the eyes of robotic systems, enabling them to perceive and understand their environment. In the context of Vision-Language-Action (VLA) models, vision systems provide the crucial input for decision-making and action execution.","sidebar":"textbookSidebar"},"module-4-vla/topic-2":{"id":"module-4-vla/topic-2","title":"Language Models for Robot Control","description":"Language models enable robots to understand and respond to natural language commands, bridging the gap between human communication and robotic action. This module covers the integration of large language models (LLMs) with robotic systems.","sidebar":"textbookSidebar"},"module-4-vla/topic-3":{"id":"module-4-vla/topic-3","title":"Action Planning and Execution","description":"Action planning and execution form the final component of Vision-Language-Action (VLA) systems, where high-level commands are translated into specific robot behaviors. This module covers planning algorithms, execution frameworks, and integration with VLA systems.","sidebar":"textbookSidebar"}}}')}}]);