---
sidebar_position: 3
title: 'Language Models for Robot Control'
---

import DiagramPlaceholder from '@site/src/components/DiagramPlaceholder';
import ProgressBar from '@site/src/components/ProgressBar';

<ProgressBar />

# Language Models for Robot Control

Language models enable robots to understand and respond to natural language commands, bridging the gap between human communication and robotic action. This module covers the integration of large language models (LLMs) with robotic systems.

## Overview of Language Model Integration

Language models for robotics typically involve:

- **Natural Language Understanding**: Parsing and interpreting commands
- **Task Planning**: Converting language to executable actions
- **Context Awareness**: Understanding the current environment and state
- **Action Generation**: Creating specific robot commands from language

## Language Model Interface Node

```python title="language_model_interface.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose, Twist
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import openai
import json
import re
from typing import Dict, List, Any, Optional

class LanguageModelInterface(Node):
    def __init__(self):
        super().__init__('language_model_interface')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Publishers and subscribers
        self.command_sub = self.create_subscription(
            String, 'robot_command', self.command_callback, 10)
        self.response_pub = self.create_publisher(
            String, 'language_response', 10)
        self.cmd_vel_pub = self.create_publisher(
            Twist, 'cmd_vel', 10)

        # Image subscription for context
        self.image_sub = self.create_subscription(
            Image, 'camera/image_raw', self.image_callback, 10)

        # Language model configuration
        self.api_key = self.declare_parameter('openai_api_key', '').get_parameter_value().string_value
        if self.api_key:
            openai.api_key = self.api_key
        else:
            self.get_logger().warn('OpenAI API key not set, using mock responses')

        # Robot state and context
        self.current_image = None
        self.robot_state = {
            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},
            'battery_level': 100.0,
            'current_task': 'idle'
        }

        # Task execution queue
        self.task_queue = []

    def command_callback(self, msg):
        """Handle incoming natural language commands"""
        command = msg.data
        self.get_logger().info(f'Received command: {command}')

        try:
            # Process command with language model
            response = self.process_command_with_llm(command)

            # Publish response
            response_msg = String()
            response_msg.data = response
            self.response_pub.publish(response_msg)

            # Execute tasks if any
            self.execute_tasks_from_response(response)

        except Exception as e:
            error_msg = f'Error processing command: {e}'
            self.get_logger().error(error_msg)
            error_response = String()
            error_response.data = error_msg
            self.response_pub.publish(error_response)

    def process_command_with_llm(self, command: str) -> str:
        """Process command using large language model"""
        if not self.api_key:
            # Mock response when API key is not available
            return self.mock_language_processing(command)

        # Prepare context for the language model
        context = self.get_robot_context()

        # Create prompt for the language model
        prompt = f"""
        You are a robot command interpreter. The robot has the following capabilities:
        - Navigation: move_forward, turn_left, turn_right, stop
        - Object manipulation: pick_up, place_down, grasp, release
        - Sensors: camera, lidar, imu
        - Current state: {context}

        User command: "{command}"

        Respond with a JSON object containing:
        1. "action" - the primary action to take
        2. "parameters" - any required parameters
        3. "explanation" - brief explanation of the interpretation
        4. "confidence" - confidence level (0-1)

        Example response: {{
            "action": "navigate_to",
            "parameters": {{"x": 1.0, "y": 2.0}},
            "explanation": "Moving to specified coordinates",
            "confidence": 0.9
        }}
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )

            # Parse the response
            content = response.choices[0].message.content
            # Extract JSON from response (in case model includes additional text)
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                parsed_response = json.loads(json_match.group())
                return json.dumps(parsed_response)
            else:
                return json.dumps({
                    "action": "unknown",
                    "parameters": {},
                    "explanation": "Could not parse model response",
                    "confidence": 0.0
                })

        except Exception as e:
            self.get_logger().error(f'LLM API error: {e}')
            return self.mock_language_processing(command)

    def mock_language_processing(self, command: str) -> str:
        """Mock language processing when API is not available"""
        # Simple rule-based parsing for demonstration
        command_lower = command.lower()

        if 'move' in command_lower or 'go' in command_lower:
            if 'forward' in command_lower:
                action = 'move_forward'
                params = {'distance': 1.0}
            elif 'backward' in command_lower:
                action = 'move_backward'
                params = {'distance': 1.0}
            elif 'left' in command_lower:
                action = 'turn_left'
                params = {'angle': 90}
            elif 'right' in command_lower:
                action = 'turn_right'
                params = {'angle': 90}
            else:
                action = 'move_to'
                params = {'x': 1.0, 'y': 1.0}
        elif 'pick' in command_lower or 'grasp' in command_lower:
            action = 'pick_up'
            params = {'object': 'unknown'}
        elif 'place' in command_lower or 'drop' in command_lower:
            action = 'place_down'
            params = {'object': 'unknown'}
        else:
            action = 'unknown'
            params = {}

        response = {
            "action": action,
            "parameters": params,
            "explanation": f"Interpreted command: {command}",
            "confidence": 0.8 if action != 'unknown' else 0.3
        }

        return json.dumps(response)

    def get_robot_context(self) -> str:
        """Get current robot context for language model"""
        context = {
            'position': self.robot_state['position'],
            'battery_level': self.robot_state['battery_level'],
            'current_task': self.robot_state['current_task'],
            'capabilities': ['navigation', 'manipulation', 'perception'],
            'environment': 'indoor'  # This could come from perception system
        }
        return json.dumps(context)

    def execute_tasks_from_response(self, response: str):
        """Execute tasks from language model response"""
        try:
            parsed_response = json.loads(response)
            action = parsed_response.get('action', 'unknown')
            params = parsed_response.get('parameters', {})

            if action == 'move_forward':
                self.execute_move_forward(params.get('distance', 1.0))
            elif action == 'move_backward':
                self.execute_move_forward(-params.get('distance', 1.0))
            elif action == 'turn_left':
                self.execute_turn(params.get('angle', 90))
            elif action == 'turn_right':
                self.execute_turn(-params.get('angle', 90))
            elif action == 'move_to':
                x = params.get('x', 0.0)
                y = params.get('y', 0.0)
                self.execute_move_to(x, y)
            elif action == 'pick_up':
                self.execute_pick_up()
            elif action == 'place_down':
                self.execute_place_down()
            else:
                self.get_logger().info(f'Unknown action: {action}')

        except json.JSONDecodeError as e:
            self.get_logger().error(f'Error parsing response JSON: {e}')
        except Exception as e:
            self.get_logger().error(f'Error executing tasks: {e}')

    def execute_move_forward(self, distance: float):
        """Execute move forward command"""
        cmd_vel = Twist()
        cmd_vel.linear.x = 0.5  # Speed in m/s
        cmd_vel.angular.z = 0.0

        # Simple movement (in practice, use action clients for precise control)
        self.cmd_vel_pub.publish(cmd_vel)
        self.get_logger().info(f'Moving forward {distance}m')

    def execute_turn(self, angle_deg: float):
        """Execute turn command"""
        cmd_vel = Twist()
        cmd_vel.linear.x = 0.0
        cmd_vel.angular.z = 0.5  # Angular speed in rad/s

        self.cmd_vel_pub.publish(cmd_vel)
        self.get_logger().info(f'Turning {angle_deg} degrees')

    def execute_move_to(self, x: float, y: float):
        """Execute move to coordinates"""
        # This would typically use navigation stack
        self.get_logger().info(f'Moving to coordinates: ({x}, {y})')

    def execute_pick_up(self):
        """Execute pick up object"""
        self.get_logger().info('Attempting to pick up object')

    def execute_place_down(self):
        """Execute place down object"""
        self.get_logger().info('Attempting to place down object')

    def image_callback(self, msg):
        """Store current image for context"""
        try:
            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().warn(f'Error processing image: {e}')

def main(args=None):
    rclpy.init(args=args)
    lang_model_node = LanguageModelInterface()

    try:
        rclpy.spin(lang_model_node)
    except KeyboardInterrupt:
        pass
    finally:
        lang_model_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Vision-Language Integration

Language models become more powerful when combined with visual input:

```python title="vision_language_integration.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import openai
import base64
import io
from PIL import Image as PILImage
import json
import numpy as np

class VisionLanguageInterface(Node):
    def __init__(self):
        super().__init__('vision_language_interface')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Publishers and subscribers
        self.command_sub = self.create_subscription(
            String, 'vision_language_command', self.command_callback, 10)
        self.image_sub = self.create_subscription(
            Image, 'camera/image_raw', self.image_callback, 10)
        self.response_pub = self.create_publisher(
            String, 'vision_language_response', 10)

        # Language model configuration
        self.api_key = self.declare_parameter('openai_api_key', '').get_parameter_value().string_value
        if self.api_key:
            openai.api_key = self.api_key
        else:
            self.get_logger().warn('OpenAI API key not set, using mock responses')

        # Store latest image
        self.latest_image = None
        self.image_available = False

    def command_callback(self, msg):
        """Handle vision-language commands"""
        command = msg.data
        self.get_logger().info(f'Received vision-language command: {command}')

        if not self.image_available:
            response = "No image available for analysis"
            response_msg = String()
            response_msg.data = response
            self.response_pub.publish(response_msg)
            return

        try:
            # Process command with vision-language model
            response = self.process_vision_language_command(command)

            # Publish response
            response_msg = String()
            response_msg.data = response
            self.response_pub.publish(response_msg)

        except Exception as e:
            error_msg = f'Error in vision-language processing: {e}'
            self.get_logger().error(error_msg)
            error_response = String()
            error_response.data = error_msg
            self.response_pub.publish(error_response)

    def process_vision_language_command(self, command: str) -> str:
        """Process command using vision-language model (GPT-4 Vision)"""
        if not self.api_key:
            # Mock response when API key is not available
            return self.mock_vision_language_processing(command)

        try:
            # Convert OpenCV image to base64
            _, buffer = cv2.imencode('.jpg', self.latest_image)
            image_base64 = base64.b64encode(buffer).decode('utf-8')

            # Create prompt for vision-language model
            response = openai.ChatCompletion.create(
                model="gpt-4-vision-preview",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": command},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500
            )

            return response.choices[0].message.content

        except Exception as e:
            self.get_logger().error(f'Vision-language model error: {e}')
            return self.mock_vision_language_processing(command)

    def mock_vision_language_processing(self, command: str) -> str:
        """Mock vision-language processing"""
        # This would analyze the image and command together
        if 'red' in command.lower():
            return "I see a red object in the scene. It appears to be a cup on the left side of the image."
        elif 'person' in command.lower():
            return "I see a person in the scene, approximately 2 meters away in front of the robot."
        elif 'navigate' in command.lower() or 'go' in command.lower():
            return "I can see a clear path forward. There is an obstacle to the right but the center path is clear."
        else:
            return f"I processed your command '{command}' with the current visual input. The scene contains various objects that I can identify and interact with."

    def image_callback(self, msg):
        """Handle incoming image"""
        try:
            self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.image_available = True
            self.get_logger().debug('Received and stored new image for vision-language processing')
        except Exception as e:
            self.get_logger().warn(f'Error processing image: {e}')
            self.image_available = False

def main(args=None):
    rclpy.init(args=args)
    vision_lang_node = VisionLanguageInterface()

    try:
        rclpy.spin(vision_lang_node)
    except KeyboardInterrupt:
        pass
    finally:
        vision_lang_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Task Planning with Language Models

```python title="language_task_planner.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from action_msgs.msg import GoalStatus
from rclpy.action import ActionClient
from geometry_msgs.msg import PoseStamped
import openai
import json
import re
from typing import List, Dict, Any

class LanguageTaskPlanner(Node):
    def __init__(self):
        super().__init__('language_task_planner')

        # Publishers and subscribers
        self.task_command_sub = self.create_subscription(
            String, 'natural_task', self.task_command_callback, 10)
        self.status_pub = self.create_publisher(
            String, 'task_status', 10)

        # Language model configuration
        self.api_key = self.declare_parameter('openai_api_key', '').get_parameter_value().string_value
        if self.api_key:
            openai.api_key = self.api_key

        # Navigation action client
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

        # Task queue
        self.task_queue = []
        self.current_task_index = 0

    def task_command_callback(self, msg):
        """Handle high-level task commands in natural language"""
        task_description = msg.data
        self.get_logger().info(f'Received task: {task_description}')

        try:
            # Parse the task using language model
            task_plan = self.generate_task_plan(task_description)

            # Execute the plan
            self.execute_task_plan(task_plan)

        except Exception as e:
            error_msg = f'Error planning task: {e}'
            self.get_logger().error(error_msg)
            status_msg = String()
            status_msg.data = error_msg
            self.status_pub.publish(status_msg)

    def generate_task_plan(self, task_description: str) -> List[Dict[str, Any]]:
        """Generate a step-by-step task plan from natural language"""
        if not self.api_key:
            return self.mock_task_planning(task_description)

        prompt = f"""
        You are a robot task planner. Convert the following natural language task into a sequence of executable actions.
        The robot has these capabilities:
        - Navigate to location (with coordinates)
        - Detect objects
        - Manipulate objects (pick/place)
        - Wait for conditions
        - Communicate status

        Task: "{task_description}"

        Return a JSON array of actions, where each action has:
        - "action": the action type (navigate, detect, manipulate, wait, communicate)
        - "parameters": required parameters for the action
        - "description": human-readable description

        Example:
        [
            {{
                "action": "navigate",
                "parameters": {{"x": 1.0, "y": 2.0, "theta": 0.0}},
                "description": "Move to kitchen area"
            }},
            {{
                "action": "detect",
                "parameters": {{"object_type": "cup"}},
                "description": "Look for a cup"
            }}
        ]
        """

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1000
            )

            content = response.choices[0].message.content
            # Extract JSON from response
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            if json_match:
                task_plan = json.loads(json_match.group())
                return task_plan
            else:
                # If no JSON found, return empty plan
                return []

        except Exception as e:
            self.get_logger().error(f'Error in task planning: {e}')
            return self.mock_task_planning(task_description)

    def mock_task_planning(self, task_description: str) -> List[Dict[str, Any]]:
        """Mock task planning when API is not available"""
        # Simple rule-based planning for demonstration
        task_plan = []

        if 'kitchen' in task_description.lower():
            task_plan.append({
                "action": "navigate",
                "parameters": {"x": 5.0, "y": 3.0, "theta": 0.0},
                "description": "Move to kitchen area"
            })

        if 'cup' in task_description.lower() or 'object' in task_description.lower():
            task_plan.append({
                "action": "detect",
                "parameters": {"object_type": "cup"},
                "description": "Look for a cup"
            })
            task_plan.append({
                "action": "manipulate",
                "parameters": {"action": "pick", "object": "cup"},
                "description": "Pick up the cup"
            })

        if 'table' in task_description.lower():
            task_plan.append({
                "action": "navigate",
                "parameters": {"x": 2.0, "y": 1.0, "theta": 1.57},
                "description": "Move to table"
            })
            task_plan.append({
                "action": "manipulate",
                "parameters": {"action": "place", "object": "cup"},
                "description": "Place the cup on table"
            })

        return task_plan

    def execute_task_plan(self, task_plan: List[Dict[str, Any]]):
        """Execute the generated task plan"""
        self.task_queue = task_plan
        self.current_task_index = 0

        if self.task_queue:
            self.execute_next_task()
        else:
            status_msg = String()
            status_msg.data = "No tasks to execute"
            self.status_pub.publish(status_msg)

    def execute_next_task(self):
        """Execute the next task in the queue"""
        if self.current_task_index >= len(self.task_queue):
            # All tasks completed
            status_msg = String()
            status_msg.data = "Task plan completed successfully"
            self.status_pub.publish(status_msg)
            return

        current_task = self.task_queue[self.current_task_index]
        self.get_logger().info(f'Executing task: {current_task["description"]}')

        # Update status
        status_msg = String()
        status_msg.data = f'Executing: {current_task["description"]}'
        self.status_pub.publish(status_msg)

        # Execute based on action type
        action_type = current_task["action"]
        if action_type == "navigate":
            self.execute_navigation_task(current_task["parameters"])
        elif action_type == "detect":
            self.execute_detection_task(current_task["parameters"])
        elif action_type == "manipulate":
            self.execute_manipulation_task(current_task["parameters"])
        elif action_type == "communicate":
            self.execute_communication_task(current_task["parameters"])

    def execute_navigation_task(self, params: Dict[str, Any]):
        """Execute navigation task"""
        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.pose.position.x = float(params.get('x', 0.0))
        goal_msg.pose.pose.position.y = float(params.get('y', 0.0))
        goal_msg.pose.pose.position.z = 0.0

        # Simple orientation (facing forward)
        from math import sin, cos
        theta = params.get('theta', 0.0)
        goal_msg.pose.pose.orientation.z = sin(theta / 2.0)
        goal_msg.pose.pose.orientation.w = cos(theta / 2.0)

        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal_msg)
        future.add_done_callback(self.navigation_done_callback)

    def navigation_done_callback(self, future):
        """Handle completion of navigation task"""
        goal_result = future.result()
        if goal_result.status == GoalStatus.STATUS_SUCCEEDED:
            self.get_logger().info('Navigation task completed successfully')
        else:
            self.get_logger().warn('Navigation task failed')

        # Move to next task
        self.current_task_index += 1
        self.execute_next_task()

def main(args=None):
    rclpy.init(args=args)
    task_planner = LanguageTaskPlanner()

    try:
        rclpy.spin(task_planner)
    except KeyboardInterrupt:
        pass
    finally:
        task_planner.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Best Practices for Language Model Integration

- **Safety**: Always validate and confirm critical commands before execution
- **Context**: Provide sufficient context to the language model for accurate interpretation
- **Fallback**: Implement fallback mechanisms when language models fail
- **Privacy**: Consider privacy implications when using cloud-based language models
- **Latency**: Optimize for real-time performance in interactive scenarios

## Summary

Language models enable natural interaction with robotic systems, allowing users to communicate with robots using everyday language. By combining language understanding with vision and action planning, robots can perform complex tasks based on high-level instructions. Proper integration requires careful consideration of safety, context, and real-time performance requirements.

<DiagramPlaceholder
  title="Vision-Language-Action Integration"
  description="Shows how language models connect perception and action in robotic systems"
/>