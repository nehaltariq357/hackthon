---
sidebar_position: 4
title: 'Action Planning and Execution'
---

import DiagramPlaceholder from '@site/src/components/DiagramPlaceholder';
import ProgressBar from '@site/src/components/ProgressBar';

<ProgressBar />

# Action Planning and Execution

Action planning and execution form the final component of Vision-Language-Action (VLA) systems, where high-level commands are translated into specific robot behaviors. This module covers planning algorithms, execution frameworks, and integration with VLA systems.

## Overview of Action Planning

Action planning in robotics involves:

- **Task Planning**: High-level task decomposition
- **Motion Planning**: Path planning and obstacle avoidance
- **Execution Monitoring**: Real-time execution and error handling
- **Replanning**: Dynamic adjustment to changing conditions

## Behavior Trees for Action Execution

Behavior trees provide a structured approach to action execution:

```python title="behavior_tree_action_planner.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose, Twist
from sensor_msgs.msg import LaserScan
from action_msgs.msg import GoalStatus
from rclpy.action import ActionClient
from geometry_msgs.msg import PoseStamped
import math
from enum import Enum
from typing import Optional, List, Dict, Any

class NodeStatus(Enum):
    SUCCESS = 1
    FAILURE = 2
    RUNNING = 3

class BehaviorNode:
    """Base class for behavior tree nodes"""
    def __init__(self, name: str):
        self.name = name
        self.status = NodeStatus.RUNNING

    def tick(self) -> NodeStatus:
        """Execute one cycle of the node"""
        raise NotImplementedError

class ActionNode(BehaviorNode):
    """Base class for action nodes"""
    def __init__(self, name: str, node: Node):
        super().__init__(name)
        self.node = node

    def tick(self) -> NodeStatus:
        """Execute the action"""
        raise NotImplementedError

class SequenceNode(BehaviorNode):
    """Sequence node executes children in order until one fails"""
    def __init__(self, name: str):
        super().__init__(name)
        self.children = []
        self.current_child_idx = 0

    def add_child(self, child: BehaviorNode):
        self.children.append(child)

    def tick(self) -> NodeStatus:
        while self.current_child_idx < len(self.children):
            child_status = self.children[self.current_child_idx].tick()

            if child_status == NodeStatus.FAILURE:
                self.current_child_idx = 0
                return NodeStatus.FAILURE
            elif child_status == NodeStatus.RUNNING:
                return NodeStatus.RUNNING
            elif child_status == NodeStatus.SUCCESS:
                self.current_child_idx += 1

        # All children succeeded
        self.current_child_idx = 0
        return NodeStatus.SUCCESS

class SelectorNode(BehaviorNode):
    """Selector node executes children until one succeeds"""
    def __init__(self, name: str):
        super().__init__(name)
        self.children = []
        self.current_child_idx = 0

    def add_child(self, child: BehaviorNode):
        self.children.append(child)

    def tick(self) -> NodeStatus:
        while self.current_child_idx < len(self.children):
            child_status = self.children[self.current_child_idx].tick()

            if child_status == NodeStatus.SUCCESS:
                self.current_child_idx = 0
                return NodeStatus.SUCCESS
            elif child_status == NodeStatus.RUNNING:
                return NodeStatus.RUNNING
            elif child_status == NodeStatus.FAILURE:
                self.current_child_idx += 1

        # All children failed
        self.current_child_idx = 0
        return NodeStatus.FAILURE

class MoveToAction(ActionNode):
    """Action to move robot to a specific pose"""
    def __init__(self, name: str, node: Node, target_pose: Pose):
        super().__init__(name, node)
        self.target_pose = target_pose
        self.nav_client = ActionClient(node, NavigateToPose, 'navigate_to_pose')
        self.goal_sent = False
        self.goal_handle = None

    def tick(self) -> NodeStatus:
        if not self.goal_sent:
            # Send navigation goal
            goal_msg = NavigateToPose.Goal()
            goal_msg.pose.header.frame_id = 'map'
            goal_msg.pose = self.target_pose

            self.nav_client.wait_for_server()
            future = self.nav_client.send_goal_async(goal_msg)
            future.add_done_callback(self.goal_response_callback)
            self.goal_sent = True
            return NodeStatus.RUNNING

        # Check if goal is still executing
        if self.goal_handle is not None:
            if self.goal_handle.status == GoalStatus.STATUS_SUCCEEDED:
                self.node.get_logger().info(f'MoveToAction {self.name} succeeded')
                return NodeStatus.SUCCESS
            elif self.goal_handle.status in [GoalStatus.STATUS_ABORTED, GoalStatus.STATUS_CANCELED]:
                self.node.get_logger().info(f'MoveToAction {self.name} failed')
                return NodeStatus.FAILURE
            else:
                return NodeStatus.RUNNING

        return NodeStatus.RUNNING

    def goal_response_callback(self, future):
        self.goal_handle = future.result()

class DetectObjectAction(ActionNode):
    """Action to detect a specific object"""
    def __init__(self, name: str, node: Node, object_type: str):
        super().__init__(name, node)
        self.object_type = object_type
        self.detected = False
        self.detection_timeout = 10.0  # seconds
        self.start_time = None

    def tick(self) -> NodeStatus:
        if self.start_time is None:
            self.start_time = self.node.get_clock().now().nanoseconds / 1e9

        # Check if object is detected (this would interface with perception system)
        # For demo, we'll simulate detection
        current_time = self.node.get_clock().now().nanoseconds / 1e9
        elapsed = current_time - self.start_time

        # Simulate detection after 2 seconds
        if elapsed > 2.0:
            self.detected = True
            self.node.get_logger().info(f'Detected {self.object_type}')
            return NodeStatus.SUCCESS

        if elapsed > self.detection_timeout:
            self.node.get_logger().info(f'Timed out detecting {self.object_type}')
            return NodeStatus.FAILURE

        return NodeStatus.RUNNING

class PickUpAction(ActionNode):
    """Action to pick up an object"""
    def __init__(self, name: str, node: Node):
        super().__init__(name, node)
        self.pick_success = False

    def tick(self) -> NodeStatus:
        # Simulate pick up action
        # In real implementation, this would interface with manipulation stack
        self.pick_success = True  # Simulate success
        self.node.get_logger().info('Pick up action executed')
        return NodeStatus.SUCCESS if self.pick_success else NodeStatus.FAILURE

class PlaceDownAction(ActionNode):
    """Action to place down an object"""
    def __init__(self, name: str, node: Node):
        super().__init__(name, node)
        self.place_success = False

    def tick(self) -> NodeStatus:
        # Simulate place down action
        self.place_success = True  # Simulate success
        self.node.get_logger().info('Place down action executed')
        return NodeStatus.SUCCESS if self.place_success else NodeStatus.FAILURE

class ActionPlannerNode(Node):
    def __init__(self):
        super().__init__('action_planner_node')

        # Publishers and subscribers
        self.task_command_sub = self.create_subscription(
            String, 'vla_task_command', self.task_command_callback, 10)
        self.status_pub = self.create_publisher(
            String, 'action_status', 10)

        # Initialize behavior tree
        self.behavior_tree = None

        # Timer for executing behavior tree
        self.tree_timer = self.create_timer(0.1, self.execute_behavior_tree)

    def task_command_callback(self, msg):
        """Handle task commands and build appropriate behavior tree"""
        command = msg.data.lower()
        self.get_logger().info(f'Received task command: {command}')

        # Build behavior tree based on command
        if 'fetch' in command or 'bring' in command:
            self.build_fetch_behavior_tree(command)
        elif 'navigate' in command or 'go to' in command:
            self.build_navigation_behavior_tree(command)
        else:
            self.get_logger().warn(f'Unknown task command: {command}')
            return

    def build_fetch_behavior_tree(self, command):
        """Build behavior tree for fetch tasks"""
        # Example: "Go to kitchen, detect cup, pick up cup, bring to table"

        # Create root selector (try different strategies)
        root = SelectorNode('fetch_strategy_selector')

        # Create main sequence
        main_sequence = SequenceNode('fetch_sequence')

        # Add subtasks to sequence
        # 1. Navigate to source location (kitchen)
        kitchen_pose = Pose()
        kitchen_pose.position.x = 5.0
        kitchen_pose.position.y = 3.0
        kitchen_pose.orientation.w = 1.0

        navigate_to_kitchen = MoveToAction('navigate_to_kitchen', self, kitchen_pose)
        main_sequence.add_child(navigate_to_kitchen)

        # 2. Detect object (cup)
        detect_cup = DetectObjectAction('detect_cup', self, 'cup')
        main_sequence.add_child(detect_cup)

        # 3. Pick up object
        pick_up = PickUpAction('pick_up', self)
        main_sequence.add_child(pick_up)

        # 4. Navigate to destination (table)
        table_pose = Pose()
        table_pose.position.x = 2.0
        table_pose.position.y = 1.0
        table_pose.orientation.w = 1.0

        navigate_to_table = MoveToAction('navigate_to_table', self, table_pose)
        main_sequence.add_child(navigate_to_table)

        # 5. Place down object
        place_down = PlaceDownAction('place_down', self)
        main_sequence.add_child(place_down)

        root.add_child(main_sequence)
        self.behavior_tree = root

        self.get_logger().info('Fetch behavior tree built')

    def build_navigation_behavior_tree(self, command):
        """Build behavior tree for navigation tasks"""
        # Create root sequence
        root = SequenceNode('navigation_sequence')

        # Parse destination from command (simplified)
        target_pose = Pose()
        if 'kitchen' in command:
            target_pose.position.x = 5.0
            target_pose.position.y = 3.0
        elif 'living room' in command:
            target_pose.position.x = 1.0
            target_pose.position.y = 2.0
        else:
            # Default to some location
            target_pose.position.x = 0.0
            target_pose.position.y = 0.0

        target_pose.orientation.w = 1.0

        # Add navigation action
        navigate_action = MoveToAction('navigate_to_location', self, target_pose)
        root.add_child(navigate_action)

        self.behavior_tree = root
        self.get_logger().info('Navigation behavior tree built')

    def execute_behavior_tree(self):
        """Execute the behavior tree"""
        if self.behavior_tree is not None:
            status = self.behavior_tree.tick()

            # Publish status
            status_msg = String()
            status_msg.data = f'Behavior tree status: {status.name}'
            self.status_pub.publish(status_msg)

def main(args=None):
    rclpy.init(args=args)
    action_planner = ActionPlannerNode()

    try:
        rclpy.spin(action_planner)
    except KeyboardInterrupt:
        pass
    finally:
        action_planner.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## State Machines for Complex Actions

State machines provide another approach to action execution:

```python title="state_machine_action_executor.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from sensor_msgs.msg import LaserScan
from enum import Enum
from typing import Dict, Any
import time

class RobotState(Enum):
    IDLE = 1
    NAVIGATING = 2
    DETECTING_OBJECT = 3
    MANIPULATING = 4
    RECHARGING = 5
    EMERGENCY_STOP = 6

class ActionExecutorNode(Node):
    def __init__(self):
        super().__init__('action_executor_node')

        # Publishers and subscribers
        self.task_command_sub = self.create_subscription(
            String, 'state_task_command', self.task_command_callback, 10)
        self.laser_sub = self.create_subscription(
            LaserScan, 'scan', self.laser_callback, 10)
        self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)
        self.status_pub = self.create_publisher(String, 'state_status', 10)

        # Initialize state
        self.current_state = RobotState.IDLE
        self.target_pose = None
        self.laser_data = None
        self.battery_level = 100.0

        # State timers
        self.state_start_time = self.get_clock().now().nanoseconds / 1e9

        # Timer for state machine execution
        self.state_machine_timer = self.create_timer(0.1, self.execute_state_machine)

    def task_command_callback(self, msg):
        """Handle task commands"""
        command = msg.data
        self.get_logger().info(f'Received state task: {command}')

        # Parse command and set target
        if 'navigate' in command.lower() or 'go to' in command.lower():
            # Extract coordinates from command (simplified)
            self.current_state = RobotState.NAVIGATING
            self.state_start_time = self.get_clock().now().nanoseconds / 1e9

    def laser_callback(self, msg):
        """Update laser data for obstacle detection"""
        self.laser_data = msg.ranges

    def execute_state_machine(self):
        """Execute state machine logic"""
        current_time = self.get_clock().now().nanoseconds / 1e9
        elapsed_time = current_time - self.state_start_time

        # Check for emergency conditions
        if self.should_enter_emergency_state():
            self.current_state = RobotState.EMERGENCY_STOP
            self.state_start_time = current_time

        # Execute current state
        if self.current_state == RobotState.IDLE:
            self.execute_idle_state()
        elif self.current_state == RobotState.NAVIGATING:
            self.execute_navigating_state()
        elif self.current_state == RobotState.DETECTING_OBJECT:
            self.execute_detecting_object_state()
        elif self.current_state == RobotState.MANIPULATING:
            self.execute_manipulating_state()
        elif self.current_state == RobotState.RECHARGING:
            self.execute_recharging_state()
        elif self.current_state == RobotState.EMERGENCY_STOP:
            self.execute_emergency_stop_state()

        # Publish status
        status_msg = String()
        status_msg.data = f'Current state: {self.current_state.name}, Battery: {self.battery_level}%'
        self.status_pub.publish(status_msg)

    def should_enter_emergency_state(self) -> bool:
        """Check if emergency state should be entered"""
        if self.laser_data:
            # Check for imminent collision (objects within 0.5m)
            min_distance = min([r for r in self.laser_data if r > 0.0], default=float('inf'))
            if min_distance < 0.5:
                return True

        # Check battery level
        if self.battery_level < 10.0:
            return True

        return False

    def execute_idle_state(self):
        """Execute idle state"""
        # Stop robot
        cmd_vel = Twist()
        cmd_vel.linear.x = 0.0
        cmd_vel.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd_vel)

        # Check if new task arrived
        # In real implementation, this would check for new commands

    def execute_navigating_state(self):
        """Execute navigation state"""
        if self.laser_data:
            # Simple obstacle avoidance
            min_distance = min([r for r in self.laser_data if r > 0.0], default=float('inf'))

            cmd_vel = Twist()
            if min_distance > 0.8:  # Safe distance
                cmd_vel.linear.x = 0.3  # Move forward
                cmd_vel.angular.z = 0.0
            elif min_distance > 0.5:  # Need to turn
                cmd_vel.linear.x = 0.1  # Slow down
                cmd_vel.angular.z = 0.5  # Turn right
            else:  # Emergency stop
                cmd_vel.linear.x = 0.0
                cmd_vel.angular.z = 0.0
                self.current_state = RobotState.EMERGENCY_STOP

            self.cmd_vel_pub.publish(cmd_vel)

    def execute_detecting_object_state(self):
        """Execute object detection state"""
        # In real implementation, this would interface with perception system
        # For demo, we'll just wait and then transition to next state
        current_time = self.get_clock().now().nanoseconds / 1e9
        elapsed_time = current_time - self.state_start_time

        if elapsed_time > 3.0:  # Detection timeout
            self.current_state = RobotState.IDLE
            self.state_start_time = current_time

    def execute_manipulating_state(self):
        """Execute manipulation state"""
        # In real implementation, this would interface with manipulation stack
        # For demo, we'll just wait and then transition to next state
        current_time = self.get_clock().now().nanoseconds / 1e9
        elapsed_time = current_time - self.state_start_time

        if elapsed_time > 2.0:  # Manipulation timeout
            self.current_state = RobotState.IDLE
            self.state_start_time = current_time

    def execute_recharging_state(self):
        """Execute recharging state"""
        # In real implementation, navigate to charging station
        # For demo, just reduce consumption
        self.battery_level += 0.1  # Simulate charging
        if self.battery_level >= 100.0:
            self.battery_level = 100.0
            self.current_state = RobotState.IDLE
            self.state_start_time = self.get_clock().now().nanoseconds / 1e9

    def execute_emergency_stop_state(self):
        """Execute emergency stop state"""
        cmd_vel = Twist()
        cmd_vel.linear.x = 0.0
        cmd_vel.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd_vel)

        # Check if emergency condition is resolved
        if self.laser_data:
            min_distance = min([r for r in self.laser_data if r > 0.0], default=float('inf'))
            if min_distance > 0.8 and self.battery_level > 10.0:
                self.current_state = RobotState.IDLE
                self.state_start_time = self.get_clock().now().nanoseconds / 1e9

def main(args=None):
    rclpy.init(args=args)
    action_executor = ActionExecutorNode()

    try:
        rclpy.spin(action_executor)
    except KeyboardInterrupt:
        pass
    finally:
        action_executor.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Integration with VLA Systems

```python title="vla_integration_node.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import Image, LaserScan
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import json
from typing import Dict, Any, Optional

class VLAIntegrationNode(Node):
    def __init__(self):
        super().__init__('vla_integration_node')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Publishers and subscribers
        self.vision_sub = self.create_subscription(
            Image, 'camera/image_raw', self.vision_callback, 10)
        self.laser_sub = self.create_subscription(
            LaserScan, 'scan', self.laser_callback, 10)
        self.language_command_sub = self.create_subscription(
            String, 'natural_language_command', self.language_callback, 10)
        self.action_status_pub = self.create_publisher(
            String, 'vla_action_status', 10)

        # Action execution components
        self.vision_data = None
        self.perception_results = {}
        self.pending_action = None
        self.action_params = {}

        # Timer for VLA execution
        self.vla_timer = self.create_timer(0.1, self.vla_execution_loop)

    def vision_callback(self, msg):
        """Process visual input"""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.vision_data = cv_image

            # Run perception pipeline
            self.perception_results = self.run_perception_pipeline(cv_image)
        except Exception as e:
            self.get_logger().error(f'Error in vision processing: {e}')

    def laser_callback(self, msg):
        """Process laser data for navigation context"""
        self.laser_ranges = msg.ranges

    def language_callback(self, msg):
        """Process natural language command"""
        try:
            # Parse command from language model
            command_data = json.loads(msg.data)
            action = command_data.get('action', 'unknown')
            params = command_data.get('parameters', {})

            self.pending_action = action
            self.action_params = params

            self.get_logger().info(f'Received VLA command: {action} with params {params}')
        except json.JSONDecodeError:
            # Handle plain text command
            command_text = msg.data
            self.get_logger().info(f'Received text command: {command_text}')
            # In a real system, this would go through the language understanding pipeline

    def run_perception_pipeline(self, image):
        """Run perception pipeline on image"""
        # This would run object detection, segmentation, etc.
        # For demo, return mock results
        results = {
            'objects': [
                {'name': 'cup', 'confidence': 0.9, 'bbox': [100, 100, 200, 200]},
                {'name': 'table', 'confidence': 0.85, 'bbox': [300, 300, 500, 400]}
            ],
            'room_type': 'kitchen',
            'navigable_areas': ['center', 'left'],
            'obstacles': []
        }
        return results

    def vla_execution_loop(self):
        """Main VLA execution loop"""
        if self.pending_action:
            # Execute pending action with current context
            success = self.execute_vla_action(self.pending_action, self.action_params)

            if success:
                self.get_logger().info(f'VLA action {self.pending_action} completed successfully')
                self.pending_action = None
                self.action_params = {}

                # Publish success status
                status_msg = String()
                status_msg.data = f'SUCCESS: {self.pending_action}'
                self.action_status_pub.publish(status_msg)
            else:
                self.get_logger().info(f'VLA action {self.pending_action} still executing')
        else:
            # Publish idle status
            status_msg = String()
            status_msg.data = 'IDLE: Waiting for VLA command'
            self.action_status_pub.publish(status_msg)

    def execute_vla_action(self, action: str, params: Dict[str, Any]) -> bool:
        """Execute VLA action with current context"""
        if action == 'navigate_to':
            return self.execute_navigation_action(params)
        elif action == 'detect_object':
            return self.execute_detection_action(params)
        elif action == 'pick_up':
            return self.execute_pickup_action(params)
        elif action == 'place_down':
            return self.execute_placement_action(params)
        elif action == 'describe_scene':
            return self.execute_description_action(params)
        else:
            self.get_logger().warn(f'Unknown VLA action: {action}')
            return True  # Mark as completed

    def execute_navigation_action(self, params: Dict[str, Any]) -> bool:
        """Execute navigation action with obstacle avoidance"""
        target_x = params.get('x', 0.0)
        target_y = params.get('y', 0.0)

        # Check if we have laser data for navigation
        if not hasattr(self, 'laser_ranges') or not self.laser_ranges:
            return False

        # Simple navigation with obstacle avoidance
        cmd_vel = Twist()

        # Check for obstacles in path
        min_distance = min([r for r in self.laser_ranges if r > 0.0], default=float('inf'))

        if min_distance < 0.5:
            # Stop and wait for path to clear
            cmd_vel.linear.x = 0.0
            cmd_vel.angular.z = 0.0
        else:
            # Move toward target (simplified)
            cmd_vel.linear.x = 0.2
            cmd_vel.angular.z = 0.0

        # Publish velocity command
        cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)
        cmd_vel_pub.publish(cmd_vel)

        # For demo, return True after some time to simulate completion
        # In real system, check if target reached
        return False  # Indicate still executing

    def execute_detection_action(self, params: Dict[str, Any]) -> bool:
        """Execute object detection action"""
        target_object = params.get('object_type', 'unknown')

        # Check if target object is in perception results
        if 'objects' in self.perception_results:
            for obj in self.perception_results['objects']:
                if obj['name'] == target_object:
                    self.get_logger().info(f'Found {target_object} with confidence {obj["confidence"]}')
                    return True

        # Object not found, return False to continue searching
        return False

    def execute_pickup_action(self, params: Dict[str, Any]) -> bool:
        """Execute pick up action"""
        target_object = params.get('object', 'unknown')

        # Check if object is in perception results and reachable
        if 'objects' in self.perception_results:
            for obj in self.perception_results['objects']:
                if obj['name'] == target_object:
                    # Simulate pickup action
                    self.get_logger().info(f'Picking up {target_object}')
                    return True

        return False

    def execute_placement_action(self, params: Dict[str, Any]) -> bool:
        """Execute placement action"""
        # Simulate placement action
        self.get_logger().info('Placing object down')
        return True

    def execute_description_action(self, params: Dict[str, Any]) -> bool:
        """Execute scene description action"""
        if self.perception_results:
            description = f"Scene contains: {[obj['name'] for obj in self.perception_results.get('objects', [])]}"
            self.get_logger().info(f'Scene description: {description}')

        return True

def main(args=None):
    rclpy.init(args=args)
    vla_node = VLAIntegrationNode()

    try:
        rclpy.spin(vla_node)
    except KeyboardInterrupt:
        pass
    finally:
        vla_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Best Practices for Action Planning

- **Modularity**: Design actions as modular, reusable components
- **Error Handling**: Implement robust error handling and recovery
- **Safety**: Include safety checks and emergency stop capabilities
- **Monitoring**: Continuously monitor execution status
- **Replanning**: Implement dynamic replanning for changing conditions

## Summary

Action planning and execution complete the Vision-Language-Action pipeline by translating high-level commands into specific robot behaviors. Behavior trees and state machines provide structured approaches to managing complex action sequences. Proper integration of these components enables robots to perform sophisticated tasks based on natural language commands while maintaining safety and reliability.

<DiagramPlaceholder
  title="Action Planning Architecture"
  description="Shows the components of action planning and execution in VLA systems"
/>