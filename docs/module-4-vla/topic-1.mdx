---
sidebar_position: 2
title: 'Vision Systems for Robotics'
---

import DiagramPlaceholder from '@site/src/components/DiagramPlaceholder';
import ProgressBar from '@site/src/components/ProgressBar';

<ProgressBar />

# Vision Systems for Robotics

Vision systems form the eyes of robotic systems, enabling them to perceive and understand their environment. In the context of Vision-Language-Action (VLA) models, vision systems provide the crucial input for decision-making and action execution.

## Overview of Robotic Vision Systems

Robotic vision systems encompass various technologies and algorithms:

- **Camera Systems**: RGB, stereo, thermal, and specialized cameras
- **Image Processing**: Filtering, enhancement, and preprocessing
- **Feature Detection**: Keypoints, edges, and object detection
- **3D Vision**: Depth estimation, stereo vision, and SLAM
- **Deep Learning**: CNNs, transformers, and specialized architectures

## Vision Processing Pipeline

Here's an example of a comprehensive vision processing pipeline for robotics:

```python title="robotic_vision_pipeline.py"
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PointStamped
from vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose
from cv_bridge import CvBridge
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from typing import List, Tuple, Optional

class RoboticVisionPipeline(Node):
    def __init__(self):
        super().__init__('robotic_vision_pipeline')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Publishers and subscribers
        self.image_sub = self.create_subscription(
            Image, 'camera/image_raw', self.image_callback, 10)
        self.camera_info_sub = self.create_subscription(
            CameraInfo, 'camera/camera_info', self.camera_info_callback, 10)

        self.detections_pub = self.create_publisher(
            Detection2DArray, 'vision/detections', 10)
        self.depth_pub = self.create_publisher(
            Image, 'vision/depth_map', 10)

        # Camera parameters
        self.camera_matrix = None
        self.distortion_coeffs = None

        # Initialize vision models
        self.initialize_models()

        # Transformation utilities
        self.transform = T.Compose([
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def initialize_models(self):
        """Initialize vision models for perception"""
        # Load pre-trained object detection model
        # In practice, this would load models like YOLO, Detectron2, etc.
        try:
            # Example using TorchVision
            self.detection_model = torch.hub.load(
                'ultralytics/yolov5', 'yolov5s', pretrained=True
            )
            self.detection_model.eval()
            self.get_logger().info('Object detection model loaded successfully')
        except Exception as e:
            self.get_logger().error(f'Failed to load detection model: {e}')

    def camera_info_callback(self, msg):
        """Update camera parameters from camera info"""
        self.camera_matrix = np.array(msg.k).reshape(3, 3)
        self.distortion_coeffs = np.array(msg.d)

    def image_callback(self, msg):
        """Process incoming camera image"""
        try:
            # Convert ROS image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Preprocess image
            processed_image = self.preprocess_image(cv_image)

            # Run object detection
            detections = self.run_object_detection(processed_image)

            # Estimate depth if stereo camera is available
            depth_map = self.estimate_depth(processed_image)

            # Publish results
            self.publish_detections(detections, msg.header)
            if depth_map is not None:
                depth_msg = self.bridge.cv2_to_imgmsg(depth_map, encoding='32FC1')
                depth_msg.header = msg.header
                self.depth_pub.publish(depth_msg)

        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """Preprocess image for vision pipeline"""
        # Undistort image if camera parameters are available
        if self.camera_matrix is not None and self.distortion_coeffs is not None:
            image = cv2.undistort(
                image, self.camera_matrix, self.distortion_coeffs
            )

        # Resize image to standard size for model input
        height, width = image.shape[:2]
        if height != 416 or width != 416:
            image = cv2.resize(image, (416, 416))

        return image

    def run_object_detection(self, image: np.ndarray) -> List[dict]:
        """Run object detection on image"""
        try:
            # Convert image for model input
            img_tensor = self.transform(image).unsqueeze(0)

            # Run detection
            with torch.no_grad():
                results = self.detection_model(img_tensor)

            # Process results
            detections = []
            for *xyxy, conf, cls in results.xyxy[0].tolist():
                if conf > 0.5:  # Confidence threshold
                    detection = {
                        'bbox': [int(coord) for coord in xyxy],
                        'confidence': conf,
                        'class_id': int(cls),
                        'class_name': self.get_class_name(int(cls))
                    }
                    detections.append(detection)

            return detections
        except Exception as e:
            self.get_logger().error(f'Error in object detection: {e}')
            return []

    def estimate_depth(self, image: np.ndarray) -> Optional[np.ndarray]:
        """Estimate depth from single image (monocular) or stereo"""
        # For stereo vision, this would use stereo matching
        # For monocular, this would use a depth estimation model
        try:
            # Placeholder for depth estimation
            # In practice, use models like MiDaS, DeepV2D, etc.
            height, width = image.shape[:2]
            depth_map = np.random.rand(height, width).astype(np.float32) * 10.0  # Placeholder

            return depth_map
        except Exception as e:
            self.get_logger().warn(f'Depth estimation failed: {e}')
            return None

    def get_class_name(self, class_id: int) -> str:
        """Get class name from class ID"""
        # COCO dataset class names
        coco_names = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',
            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',
            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',
            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',
            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',
            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',
            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',
            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',
            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',
            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',
            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
            'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]

        if 0 <= class_id < len(coco_names):
            return coco_names[class_id]
        else:
            return f'unknown_{class_id}'

    def publish_detections(self, detections: List[dict], header):
        """Publish detections as ROS message"""
        detections_msg = Detection2DArray()
        detections_msg.header = header

        for detection in detections:
            detection_msg = Detection2D()
            detection_msg.header = header

            # Bounding box
            bbox = BoundingBox2D()
            bbox.center.position.x = (detection['bbox'][0] + detection['bbox'][2]) / 2
            bbox.center.position.y = (detection['bbox'][1] + detection['bbox'][3]) / 2
            bbox.size_x = detection['bbox'][2] - detection['bbox'][0]
            bbox.size_y = detection['bbox'][3] - detection['bbox'][1]
            detection_msg.bbox = bbox

            # Results
            result = ObjectHypothesisWithPose()
            result.hypothesis.class_id = detection['class_name']
            result.hypothesis.score = detection['confidence']
            detection_msg.results.append(result)

            detections_msg.detections.append(detection_msg)

        self.detections_pub.publish(detections_msg)
        self.get_logger().info(f'Published {len(detections)} detections')

def main(args=None):
    rclpy.init(args=args)
    vision_pipeline = RoboticVisionPipeline()

    try:
        rclpy.spin(vision_pipeline)
    except KeyboardInterrupt:
        pass
    finally:
        vision_pipeline.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Advanced Vision Techniques

### Stereo Vision for Depth Perception

```python title="stereo_vision_node.py"
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from sensor_msgs.msg import CameraInfo
from cv_bridge import CvBridge
import cv2
import numpy as np

class StereoVisionNode(Node):
    def __init__(self):
        super().__init__('stereo_vision_node')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Subscribers for stereo pair
        self.left_sub = self.create_subscription(
            Image, 'stereo/left/image_rect', self.left_callback, 10)
        self.right_sub = self.create_subscription(
            Image, 'stereo/right/image_rect', self.right_callback, 10)

        # Camera info subscribers
        self.left_info_sub = self.create_subscription(
            CameraInfo, 'stereo/left/camera_info', self.left_info_callback, 10)
        self.right_info_sub = self.create_subscription(
            CameraInfo, 'stereo/right/camera_info', self.right_info_callback, 10)

        # Publisher for disparity map
        self.disparity_pub = self.create_publisher(Image, 'stereo/disparity', 10)

        # Stereo matcher
        self.stereo_matcher = cv2.StereoSGBM_create(
            minDisparity=0,
            numDisparities=128,  # Must be divisible by 16
            blockSize=5,
            P1=8 * 3 * 5**2,
            P2=32 * 3 * 5**2,
            disp12MaxDiff=1,
            uniquenessRatio=15,
            speckleWindowSize=0,
            speckleRange=2,
            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
        )

        # Camera parameters
        self.left_camera_matrix = None
        self.right_camera_matrix = None
        self.rotation_matrix = None
        self.translation_vector = None

        # Store latest images
        self.latest_left = None
        self.latest_right = None

    def left_info_callback(self, msg):
        """Handle left camera info"""
        self.left_camera_matrix = np.array(msg.k).reshape(3, 3)

    def right_info_callback(self, msg):
        """Handle right camera info"""
        self.right_camera_matrix = np.array(msg.k).reshape(3, 3)

    def left_callback(self, msg):
        """Handle left camera image"""
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')
        self.latest_left = cv_image

        # Process stereo pair if both images are available
        if self.latest_right is not None:
            self.process_stereo_pair()

    def right_callback(self, msg):
        """Handle right camera image"""
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')
        self.latest_right = cv_image

        # Process stereo pair if both images are available
        if self.latest_left is not None:
            self.process_stereo_pair()

    def process_stereo_pair(self):
        """Process stereo image pair to generate disparity map"""
        if self.latest_left is None or self.latest_right is None:
            return

        # Compute disparity
        disparity = self.stereo_matcher.compute(
            self.latest_left, self.latest_right
        ).astype(np.float32)

        # Convert to depth map
        disparity = disparity / 16.0  # SGBM returns 16x disparity values

        # Publish disparity map
        disparity_msg = self.bridge.cv2_to_imgmsg(disparity, encoding='32FC1')
        disparity_msg.header.stamp = self.get_clock().now().to_msg()
        disparity_msg.header.frame_id = 'stereo_link'
        self.disparity_pub.publish(disparity_msg)

        self.get_logger().info('Published disparity map')

def main(args=None):
    rclpy.init(args=args)
    stereo_node = StereoVisionNode()

    try:
        rclpy.spin(stereo_node)
    except KeyboardInterrupt:
        pass
    finally:
        stereo_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Visual SLAM Integration

```python title="visual_slam_node.py"
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, Imu
from geometry_msgs.msg import PoseStamped, TransformStamped
from cv_bridge import CvBridge
import cv2
import numpy as np
from tf2_ros import TransformBroadcaster
import tf_transformations

class VisualSLAMNode(Node):
    def __init__(self):
        super().__init__('visual_slam_node')

        # Initialize CV Bridge
        self.bridge = CvBridge()

        # Publisher for robot pose
        self.pose_pub = self.create_publisher(PoseStamped, 'visual_slam/pose', 10)

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, 'camera/image_raw', self.image_callback, 10)

        # Transform broadcaster
        self.tf_broadcaster = TransformBroadcaster(self)

        # ORB feature detector
        self.orb = cv2.ORB_create(nfeatures=1000)
        self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)

        # SLAM state
        self.previous_keypoints = None
        self.previous_descriptors = None
        self.current_position = np.array([0.0, 0.0, 0.0])
        self.current_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion

        # Previous image for motion estimation
        self.previous_image = None

    def image_callback(self, msg):
        """Process incoming image for SLAM"""
        try:
            # Convert ROS image to OpenCV
            current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')

            if self.previous_image is not None:
                # Extract features
                current_keypoints, current_descriptors = self.extract_features(current_image)

                if self.previous_keypoints is not None and current_keypoints is not None:
                    # Match features
                    matches = self.match_features(
                        self.previous_descriptors, current_descriptors
                    )

                    if len(matches) >= 10:  # Minimum matches for reliable pose estimation
                        # Estimate motion
                        motion = self.estimate_motion(
                            self.previous_keypoints, current_keypoints, matches
                        )

                        if motion is not None:
                            # Update position
                            self.update_position(motion)

                            # Publish pose
                            self.publish_pose(msg.header)

            # Store current data for next iteration
            self.previous_image = current_image
            self.previous_keypoints = self.extract_features(current_image)[0]
            self.previous_descriptors = self.extract_features(current_image)[1]

        except Exception as e:
            self.get_logger().error(f'Error in SLAM processing: {e}')

    def extract_features(self, image):
        """Extract ORB features from image"""
        keypoints = self.orb.detect(image, None)
        keypoints, descriptors = self.orb.compute(image, keypoints)
        return keypoints, descriptors

    def match_features(self, desc1, desc2):
        """Match features between two images"""
        if desc1 is None or desc2 is None:
            return []

        try:
            matches = self.bf.match(desc1, desc2)
            matches = sorted(matches, key=lambda x: x.distance)
            return matches
        except Exception as e:
            self.get_logger().warn(f'Feature matching failed: {e}')
            return []

    def estimate_motion(self, kp1, kp2, matches):
        """Estimate motion between two frames"""
        if len(matches) < 10:
            return None

        # Get matched keypoints
        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

        # Compute homography
        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

        if homography is not None:
            # Extract translation from homography (simplified)
            dx = homography[0, 2]
            dy = homography[1, 2]

            # Convert to reasonable scale (this is simplified)
            translation = np.array([dx * 0.01, dy * 0.01, 0.0])  # Scale factor is arbitrary
            return translation

        return None

    def update_position(self, motion):
        """Update robot position based on estimated motion"""
        self.current_position += motion

    def publish_pose(self, header):
        """Publish current pose and broadcast transform"""
        # Create pose message
        pose_msg = PoseStamped()
        pose_msg.header = header
        pose_msg.pose.position.x = float(self.current_position[0])
        pose_msg.pose.position.y = float(self.current_position[1])
        pose_msg.pose.position.z = float(self.current_position[2])
        pose_msg.pose.orientation.x = float(self.current_orientation[0])
        pose_msg.pose.orientation.y = float(self.current_orientation[1])
        pose_msg.pose.orientation.z = float(self.current_orientation[2])
        pose_msg.pose.orientation.w = float(self.current_orientation[3])

        self.pose_pub.publish(pose_msg)

        # Broadcast transform
        t = TransformStamped()
        t.header.stamp = self.get_clock().now().to_msg()
        t.header.frame_id = 'map'
        t.child_frame_id = 'visual_slam_frame'
        t.transform.translation.x = float(self.current_position[0])
        t.transform.translation.y = float(self.current_position[1])
        t.transform.translation.z = float(self.current_position[2])
        t.transform.rotation.x = float(self.current_orientation[0])
        t.transform.rotation.y = float(self.current_orientation[1])
        t.transform.rotation.z = float(self.current_orientation[2])
        t.transform.rotation.w = float(self.current_orientation[3])

        self.tf_broadcaster.sendTransform(t)

def main(args=None):
    rclpy.init(args=args)
    slam_node = VisualSLAMNode()

    try:
        rclpy.spin(slam_node)
    except KeyboardInterrupt:
        pass
    finally:
        slam_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Best Practices for Vision Systems

- **Lighting Conditions**: Account for varying lighting in algorithm design
- **Real-time Processing**: Optimize algorithms for real-time performance
- **Calibration**: Regularly calibrate cameras and vision systems
- **Robustness**: Design systems to handle partial occlusions and noise
- **Validation**: Test vision systems under various conditions

## Summary

Vision systems provide robotic systems with the ability to perceive and understand their environment. By implementing robust vision processing pipelines, feature detection, and depth estimation, robots can navigate and interact with the world effectively. These capabilities form the foundation for higher-level Vision-Language-Action systems.

<DiagramPlaceholder
  title="Robotic Vision System Architecture"
  description="Shows the components of a robotic vision system from cameras to perception outputs"
/>